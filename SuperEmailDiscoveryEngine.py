#!/usr/bin/env python3
"""
è¶…çº§é‚®ç®±æœç´¢å¼•æ“ - åŸºäº2024å¹´æœ€ä½³å®è·µ
- ä½¿ç”¨æœ€æœ‰æ•ˆçš„Googleæœç´¢æ“ä½œç¬¦
- åŸºäºä¸“ä¸šé‚®ç®±å‘ç°å·¥å…·çš„ç­–ç•¥
- æŒç»­æœç´¢ç›´åˆ°æ‰¾åˆ°çœŸå®é‚®ç®±
- è¯¦ç»†æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§
"""

import sys
import json
import time
import re
import requests
import os
from datetime import datetime
from urllib.parse import quote, urlencode
from bs4 import BeautifulSoup
import concurrent.futures
import logging

class SuperEmailDiscoveryEngine:
    def __init__(self):
        self.setup_logging()

        # SearxNGé…ç½® - Railwayå…¼å®¹
        self.searxng_url = os.environ.get('SEARXNG_URL', 'http://localhost:8080')
        
        # ç½‘ç»œä¼šè¯é…ç½® - æ— è¶…æ—¶é™åˆ¶
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': 'https://www.google.com/',
            'Connection': 'keep-alive'
        })
        # è®¾ç½®æ— é™è¶…æ—¶
        self.session.timeout = None
        
        # é‚®ç®±æ¨¡å¼
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        
        # æœç´¢çŠ¶æ€
        self.found_emails = []
        self.search_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'emails_found': 0,
            'websites_scraped': 0,
            'unique_domains': set(),
            'query_success_rate': {}
        }
        
        self.logger.info("ğŸš€ è¶…çº§é‚®ç®±æœç´¢å¼•æ“åˆå§‹åŒ–")
        self.logger.info("   ğŸ“Š åŸºäº2024å¹´æœ€ä½³é‚®ç®±å‘ç°å®è·µ")
        self.logger.info("   ğŸ¯ ç›®æ ‡ï¼šç¡®ä¿æ‰¾åˆ°çœŸå®æœ‰æ•ˆçš„é‚®ç®±åœ°å€")
        
    def setup_logging(self):
        """è®¾ç½®è¯¦ç»†æ—¥å¿—"""
        self.logger = logging.getLogger('SuperEmailEngine')
        self.logger.setLevel(logging.INFO)
        
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        file_handler = logging.FileHandler('super_email_discovery.log', encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )
        
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)
        
        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)
    
    def generate_professional_search_strategies(self, industry, round_num=1):
        """ç”ŸæˆåŸºäº2024å¹´æœ€ä½³å®è·µçš„ä¸“ä¸šæœç´¢ç­–ç•¥"""
        self.logger.info(f"ğŸ§  ç”Ÿæˆç¬¬{round_num}è½®ä¸“ä¸šæœç´¢ç­–ç•¥ - {industry}")
        
        # åŸºäºç ”ç©¶çš„æœ€æœ‰æ•ˆæœç´¢ç­–ç•¥
        base_strategies = []
        
        if round_num == 1:
            # ç¬¬ä¸€è½®ï¼šç®€çŸ­é«˜æ•ˆæœç´¢æ¨¡å¼
            base_strategies = [
                f'{industry} email contact',
                f'{industry} CEO email',
                f'{industry} founder contact',
                f'{industry} business email',
                f'{industry} company contact'
            ]
        elif round_num == 2:
            # ç¬¬äºŒè½®ï¼šç®€çŸ­å˜ä½“æœç´¢
            base_strategies = [
                f'{industry} team email',
                f'{industry} sales contact',
                f'{industry} support email',
                f'{industry} info contact',
                f'{industry} director email'
            ]
        elif round_num == 3:
            # ç¬¬ä¸‰è½®ï¼šèŒä½ç›¸å…³æœç´¢
            base_strategies = [
                f'{industry} manager email',
                f'{industry} consultant contact',
                f'{industry} specialist email',
                f'{industry} expert contact',
                f'{industry} advisor email'
            ]
        else:
            # åç»­è½®æ¬¡ï¼šç®€çŸ­æ‰©å±•æœç´¢
            base_strategies = [
                f'{industry} startup email',
                f'{industry} company email',
                f'{industry} business contact',
                f'{industry} executive email',
                f'{industry} owner contact'
            ]
        
        self.logger.info(f"   âœ… ç”Ÿæˆ{len(base_strategies)}ä¸ªä¸“ä¸šçº§æœç´¢ç­–ç•¥")
        return base_strategies
    
    def search_with_advanced_logging(self, query, max_results=50):
        """é«˜çº§SearxNGæœç´¢ - æ— è¶…æ—¶é™åˆ¶ï¼Œå°½å¯èƒ½å¤šåœ°è·å–ç»“æœ"""
        try:
            self.logger.info(f"ğŸ” æ·±åº¦ä¸“ä¸šæœç´¢: {query[:80]}...")
            self.search_stats['total_queries'] += 1
            
            params = {
                'q': query,
                'format': 'json',
                'categories': 'general',
                'pageno': 1
            }
            
            start_time = time.time()
            # ç§»é™¤è¶…æ—¶é™åˆ¶ - è®©æœç´¢æœ‰è¶³å¤Ÿæ—¶é—´å®Œæˆ
            response = self.session.get(f"{self.searxng_url}/search", params=params)
            duration = time.time() - start_time
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    results = data.get('results', [])
                    
                    self.logger.info(f"   âœ… æœç´¢æˆåŠŸ ({duration:.1f}s): {len(results)}ä¸ªç»“æœ")
                    self.search_stats['successful_queries'] += 1
                    
                    # åˆ†æç»“æœè´¨é‡
                    email_indicators = 0
                    contact_indicators = 0
                    
                    for result in results:
                        text = f"{result.get('title', '')} {result.get('content', '')}".lower()
                        if '@' in text:
                            email_indicators += 1
                        if any(word in text for word in ['contact', 'email', 'reach']):
                            contact_indicators += 1
                    
                    self.logger.info(f"   ğŸ“Š è´¨é‡åˆ†æ: {email_indicators}ä¸ª@ç¬¦å·, {contact_indicators}ä¸ªè”ç³»æŒ‡ç¤ºå™¨")
                    
                    # è®°å½•æŸ¥è¯¢æˆåŠŸç‡
                    query_type = self.classify_query_type(query)
                    if query_type not in self.search_stats['query_success_rate']:
                        self.search_stats['query_success_rate'][query_type] = {'success': 0, 'total': 0}
                    
                    self.search_stats['query_success_rate'][query_type]['total'] += 1
                    if email_indicators > 0:
                        self.search_stats['query_success_rate'][query_type]['success'] += 1
                    
                    return results[:max_results]
                    
                except json.JSONDecodeError as e:
                    self.logger.error(f"   âŒ JSONè§£æå¤±è´¥: {str(e)}")
                    return []
            else:
                self.logger.error(f"   âŒ æœç´¢è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"   âŒ æœç´¢é”™è¯¯: {str(e)}")
            return []
    
    def classify_query_type(self, query):
        """åˆ†ç±»æœç´¢æŸ¥è¯¢ç±»å‹ä»¥è¿›è¡Œæ€§èƒ½åˆ†æ"""
        query_lower = query.lower()
        if 'site:linkedin.com' in query_lower:
            return 'linkedin_search'
        elif 'site:' in query_lower:
            return 'site_specific'
        elif 'filetype:' in query_lower:
            return 'file_search'
        elif 'intext:' in query_lower:
            return 'content_search'
        else:
            return 'general_search'
    
    def extract_emails_advanced(self, text, source=""):
        """é«˜çº§é‚®ç®±æå– - ä½¿ç”¨2024å¹´æœ€ä½³æ¨¡å¼"""
        if not text:
            return []
            
        # æ‰¾åˆ°æ‰€æœ‰æ½œåœ¨é‚®ç®±
        potential_emails = self.email_pattern.findall(text)
        
        valid_emails = []
        excluded_count = 0
        
        for email in potential_emails:
            email_lower = email.lower()
            
            # 2024å¹´æ›´æ–°çš„æ’é™¤è§„åˆ™
            exclusions = [
                'example.com', 'test.com', 'domain.com', 'yoursite.com', 'company.com',
                'noreply', 'no-reply', 'donotreply', 'bounce', 'mailer-daemon',
                'privacy@', 'legal@', 'abuse@', 'postmaster@', 'webmaster@',
                'support@example', 'admin@example', 'info@example', 'sales@example',
                'sample@', 'demo@', 'fake@', 'null@', 'void@'
            ]
            
            if any(pattern in email_lower for pattern in exclusions):
                excluded_count += 1
                continue
            
            # éªŒè¯é‚®ç®±æ ¼å¼
            if self.validate_email_format(email):
                valid_emails.append(email)
                domain = email.split('@')[1]
                self.search_stats['unique_domains'].add(domain)
                self.logger.info(f"   âœ… å‘ç°æœ‰æ•ˆé‚®ç®±: {email} (æ¥æº: {source[:30]})")
        
        if excluded_count > 0:
            self.logger.debug(f"   ğŸ—‘ï¸ æ’é™¤äº†{excluded_count}ä¸ªç¤ºä¾‹/æ— æ•ˆé‚®ç®±")
        
        return list(set(valid_emails))
    
    def validate_email_format(self, email):
        """éªŒè¯é‚®ç®±æ ¼å¼"""
        if not (5 < len(email) < 100 and email.count('@') == 1):
            return False
        
        local, domain = email.split('@')
        
        # æ£€æŸ¥æœ¬åœ°éƒ¨åˆ†
        if not local or len(local) > 64:
            return False
        
        # æ£€æŸ¥åŸŸåéƒ¨åˆ†
        if not domain or '.' not in domain or len(domain) < 4:
            return False
        
        # æ£€æŸ¥é¡¶çº§åŸŸå
        tld = domain.split('.')[-1]
        if len(tld) < 2 or not tld.isalpha():
            return False
        
        return True
    
    def scrape_website_advanced(self, url):
        """é«˜çº§ç½‘ç«™çˆ¬å– - ä¸“æ³¨è”ç³»ä¿¡æ¯ï¼Œæ— æ—¶é—´é™åˆ¶"""
        try:
            self.logger.info(f"   ğŸŒ æ·±åº¦æ— é™çˆ¬å–: {url[:60]}...")
            self.search_stats['websites_scraped'] += 1
            
            start_time = time.time()
            # ç§»é™¤è¶…æ—¶é™åˆ¶ - è®©çˆ¬å–æœ‰å……è¶³æ—¶é—´
            response = self.session.get(url)
            duration = time.time() - start_time
            
            if response.status_code != 200:
                self.logger.warning(f"   âš ï¸ HTTP {response.status_code}: {url}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤å¹²æ‰°å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()
            
            # ä¼˜å…ˆæœç´¢è”ç³»ç›¸å…³åŒºåŸŸ
            priority_areas = []
            
            # æŸ¥æ‰¾è”ç³»é¡µé¢å…³é”®åŒºåŸŸ
            contact_selectors = [
                '[class*="contact"]', '[id*="contact"]',
                '[class*="about"]', '[id*="about"]', 
                '[class*="team"]', '[id*="team"]',
                '[class*="staff"]', '[id*="staff"]',
                '[class*="press"]', '[id*="press"]',
                '[class*="media"]', '[id*="media"]'
            ]
            
            for selector in contact_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    priority_areas.append(elem.get_text())
            
            # è·å–ä¸»è¦å†…å®¹
            main_content = soup.get_text()
            
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ï¼Œä¼˜å…ˆå¤„ç†è”ç³»åŒºåŸŸ
            all_text = ' '.join(priority_areas) + ' ' + main_content
            
            emails = self.extract_emails_advanced(all_text, f"ç½‘ç«™ {url}")
            
            self.logger.info(f"   âœ… çˆ¬å–å®Œæˆ ({duration:.1f}s): {len(emails)}ä¸ªé‚®ç®±")
            return emails
            
        except Exception as e:
            self.logger.error(f"   âŒ çˆ¬å–å¤±è´¥ {url}: {str(e)}")
            return []
    
    def execute_persistent_discovery(self, industry, target_count=5, max_rounds=20):
        """æ‰§è¡Œæ— é™åˆ¶æŒç»­æœç´¢ - è¶Šå¤šè¶Šå‡†ç¡®"""
        self.logger.info(f"ğŸš€ å¯åŠ¨æ— é™åˆ¶è¶…çº§é‚®ç®±æœç´¢ - {industry}")
        self.logger.info(f"   ğŸ¯ ç›®æ ‡: {target_count}ä¸ªé‚®ç®±")
        self.logger.info(f"   ğŸ”„ æœ€å¤§è½®æ•°: {max_rounds} (å¤§å¹…å¢åŠ )")
        self.logger.info(f"   ğŸ“Š ä½¿ç”¨2024å¹´æœ€ä½³æœç´¢å®è·µ")
        self.logger.info(f"   â° æ— æ—¶é—´é™åˆ¶ - æœç´¢è¶Šå¤šè¶Šå‡†ç¡®")
        
        start_time = time.time()
        all_emails = []
        round_num = 1
        consecutive_empty_rounds = 0
        
        while len(all_emails) < target_count and round_num <= max_rounds:
            self.logger.info(f"\nğŸ“ ç¬¬{round_num}è½®æœç´¢ (å·²æ‰¾åˆ° {len(all_emails)}/{target_count})")
            
            # ç”Ÿæˆæœ¬è½®ç­–ç•¥
            strategies = self.generate_professional_search_strategies(industry, round_num)
            round_emails = []
            
            for i, strategy in enumerate(strategies, 1):
                self.logger.info(f"   ğŸ¯ ç­–ç•¥{i}/{len(strategies)}: {strategy[:70]}...")
                
                # æœç´¢
                results = self.search_with_advanced_logging(strategy)
                
                if not results:
                    self.logger.warning(f"   âš ï¸ ç­–ç•¥{i} æ— ç»“æœ")
                    continue
                
                # ä»æœç´¢é¢„è§ˆæå–é‚®ç®±
                preview_emails = []
                for result in results:
                    text = f"{result.get('title', '')} {result.get('content', '')}"
                    emails = self.extract_emails_advanced(text, f"æœç´¢é¢„è§ˆ {i}")
                    
                    for email in emails:
                        if not any(e['email'] == email for e in preview_emails):
                            preview_emails.append({
                                'email': email,
                                'source': 'search_preview',
                                'source_url': result.get('url', ''),
                                'source_title': result.get('title', ''),
                                'confidence': 0.8,
                                'round': round_num,
                                'strategy': strategy,
                                'discovery_method': 'professional_search'
                            })
                
                round_emails.extend(preview_emails)
                self.logger.info(f"   ğŸ“§ ç­–ç•¥{i}é¢„è§ˆ: {len(preview_emails)}ä¸ªé‚®ç®±")
                
                # å¹¶è¡Œçˆ¬å–æ›´å¤šç½‘ç«™ - æ— é™åˆ¶æ¨¡å¼
                promising_sites = [r for r in results[:20] 
                                 if any(word in r.get('url', '').lower() 
                                       for word in ['contact', 'about', 'team', 'press'])]
                
                if not promising_sites:
                    promising_sites = results[:15]  # å¢åŠ å¤‡é€‰æ–¹æ¡ˆæ•°é‡
                
                self.logger.info(f"   ğŸŒ æ·±åº¦å¹¶è¡Œçˆ¬å–{len(promising_sites)}ä¸ªç½‘ç«™ (æ— æ—¶é—´é™åˆ¶)...")
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
                    future_to_result = {
                        executor.submit(self.scrape_website_advanced, site['url']): site 
                        for site in promising_sites
                    }
                    
                    # ç§»é™¤è¶…æ—¶é™åˆ¶ï¼Œè®©æ‰€æœ‰ç½‘ç«™éƒ½æœ‰å……è¶³æ—¶é—´å®Œæˆ
                    for future in concurrent.futures.as_completed(future_to_result):
                        try:
                            site = future_to_result[future]
                            website_emails = future.result()
                            
                            for email in website_emails:
                                if not any(e['email'] == email for e in round_emails):
                                    round_emails.append({
                                        'email': email,
                                        'source': 'website_scraping',
                                        'source_url': site['url'],
                                        'source_title': site.get('title', ''),
                                        'confidence': 0.95,
                                        'round': round_num,
                                        'strategy': strategy,
                                        'discovery_method': 'deep_scraping'
                                    })
                        except Exception as e:
                            continue
                
                # æ£€æŸ¥è¿›åº¦ï¼Œä½†ä¸ç«‹å³åœæ­¢ - è®©å®ƒç»§ç»­æœç´¢æ›´å¤š
                all_unique = {e['email']: e for e in all_emails + round_emails}
                if len(all_unique) >= target_count:
                    self.logger.info(f"ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡ï¼Œä½†ç»§ç»­æœç´¢ä»¥è·å¾—æ›´å‡†ç¡®ç»“æœ...")
                    # ä¸breakï¼Œç»§ç»­æœç´¢
                
                time.sleep(0.3)  # å‡å°‘ç­–ç•¥é—´éš”
            
            # æ›´æ–°æ€»é‚®ç®±åˆ—è¡¨
            all_emails.extend(round_emails)
            all_unique = {e['email']: e for e in all_emails}
            all_emails = list(all_unique.values())
            
            self.logger.info(f"ğŸ“Š ç¬¬{round_num}è½®ç»“æœ: æ–°å¢{len(round_emails)}ä¸ªï¼Œæ€»è®¡{len(all_emails)}ä¸ªå”¯ä¸€é‚®ç®±")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´ç­–ç•¥ï¼Œä½†ä¸è½»æ˜“æ”¾å¼ƒ
            if len(round_emails) == 0:
                consecutive_empty_rounds += 1
                self.logger.warning(f"âš ï¸ è¿ç»­{consecutive_empty_rounds}è½®æ— ç»“æœ - ç»§ç»­å°è¯•")
                
                if consecutive_empty_rounds >= 5:  # å¢åŠ å®¹å¿åº¦
                    self.logger.info("ğŸ”„ åˆ‡æ¢åˆ°æ›´å¹¿æ³›çš„æœç´¢ç­–ç•¥...")
            else:
                consecutive_empty_rounds = 0
            
            # å³ä½¿è¾¾åˆ°ç›®æ ‡ä¹Ÿä¸ç«‹å³é€€å‡º - ç»§ç»­æœç´¢è·å¾—æ›´å¤šé‚®ç®±
            if len(all_emails) >= target_count and round_num >= 5:
                self.logger.info(f"ğŸ¯ å·²æ”¶é›†è¶³å¤Ÿé‚®ç®±å¹¶è¿›è¡Œäº†å……åˆ†æœç´¢ï¼Œå‡†å¤‡ç»“æŸ")
                break
            
            round_num += 1
            if round_num <= max_rounds:
                time.sleep(1)  # å‡å°‘è½®æ¬¡é—´éš”
        
        # æ•´ç†æœ€ç»ˆç»“æœ
        final_emails = all_emails[:target_count]
        total_time = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡
        self.search_stats['emails_found'] = len(final_emails)
        
        self.logger.info(f"\nğŸŠ è¶…çº§æœç´¢å®Œæˆï¼")
        self.logger.info(f"   ğŸ“§ æœ€ç»ˆé‚®ç®±: {len(final_emails)}ä¸ª")
        self.logger.info(f"   ğŸ”„ æœç´¢è½®æ•°: {round_num-1}")
        self.logger.info(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’")
        self.logger.info(f"   ğŸ“Š æˆåŠŸç‡: {self.search_stats['successful_queries']}/{self.search_stats['total_queries']}")
        self.logger.info(f"   ğŸŒ çˆ¬å–ç½‘ç«™: {self.search_stats['websites_scraped']}ä¸ª")
        self.logger.info(f"   ğŸ¢ å‘ç°åŸŸå: {len(self.search_stats['unique_domains'])}ä¸ª")
        
        # æ˜¾ç¤ºå‘ç°çš„é‚®ç®±
        if final_emails:
            self.logger.info("\nğŸ“§ å‘ç°çš„é‚®ç®±åœ°å€:")
            for i, email_data in enumerate(final_emails, 1):
                self.logger.info(f"   {i}. {email_data['email']} (ç½®ä¿¡åº¦: {email_data['confidence']})")
        
        return {
            'success': len(final_emails) > 0,
            'emails': [e['email'] for e in final_emails],
            'email_details': final_emails,
            'total_emails': len(final_emails),
            'search_rounds': round_num - 1,
            'execution_time': total_time,
            'search_stats': self.prepare_stats_for_json(),
            'industry': industry,
            'target_achieved': len(final_emails) >= target_count,
            'method': 'super_email_discovery_2024',
            'confidence_score': sum(e['confidence'] for e in final_emails) / len(final_emails) if final_emails else 0,
            'timestamp': datetime.now().isoformat()
        }
    
    def prepare_stats_for_json(self):
        """å‡†å¤‡ç»Ÿè®¡æ•°æ®ç”¨äºJSONåºåˆ—åŒ–"""
        stats = dict(self.search_stats)
        stats['unique_domains'] = list(self.search_stats['unique_domains'])
        return stats

def main():
    if len(sys.argv) < 2:
        print('ä½¿ç”¨æ–¹æ³•: python3 SuperEmailDiscoveryEngine.py "è¡Œä¸šåç§°" [é‚®ç®±æ•°é‡]')
        print('ç¤ºä¾‹: python3 SuperEmailDiscoveryEngine.py "AI startup" 5')
        return
    
    industry = sys.argv[1]
    target_count = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    engine = SuperEmailDiscoveryEngine()
    results = engine.execute_persistent_discovery(industry, target_count)
    
    print("\n" + "="*90)
    print("ğŸ¯ è¶…çº§é‚®ç®±æœç´¢å¼•æ“ - æœ€ç»ˆæŠ¥å‘Š")
    print("="*90)
    
    if results['success']:
        print(f"âœ… æˆåŠŸå‘ç° {results['total_emails']} ä¸ªé‚®ç®±åœ°å€:")
        for i, email in enumerate(results['emails'], 1):
            print(f"   {i}. {email}")
        
        print(f"\nğŸ“Š æœç´¢æ€§èƒ½æŒ‡æ ‡:")
        print(f"   ğŸ”„ æœç´¢è½®æ•°: {results['search_rounds']}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {results['execution_time']:.1f}ç§’")
        print(f"   ğŸ¯ ç›®æ ‡è¾¾æˆ: {'æ˜¯' if results['target_achieved'] else 'å¦'}")
        print(f"   ğŸ“ˆ æŸ¥è¯¢æˆåŠŸç‡: {results['search_stats']['successful_queries']}/{results['search_stats']['total_queries']}")
        print(f"   ğŸŒ ç½‘ç«™çˆ¬å–: {results['search_stats']['websites_scraped']}ä¸ª")
        print(f"   ğŸ¢ å‘ç°åŸŸå: {len(results['search_stats']['unique_domains'])}ä¸ª")
        print(f"   ğŸ­ å¹³å‡ç½®ä¿¡åº¦: {results['confidence_score']:.2f}")
        
    else:
        print("âŒ æœªèƒ½å‘ç°é‚®ç®±åœ°å€")
        print("ğŸ’¡ å»ºè®®ï¼šå°è¯•æ›´å…·ä½“çš„è¡Œä¸šæè¿°æˆ–å¢åŠ æœç´¢è½®æ•°")
    
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ (JSON):")
    print(json.dumps(results, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()