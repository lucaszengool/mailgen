#!/usr/bin/env python3
"""
è¶…çº§é‚®ç®±æœç´¢å¼•æ“ - åŸºäº2024å¹´æœ€ä½³å®è·µ
- ä½¿ç”¨æœ€æœ‰æ•ˆçš„Googleæœç´¢æ“ä½œç¬¦
- åŸºäºä¸“ä¸šé‚®ç®±å‘ç°å·¥å…·çš„ç­–ç•¥
- æŒç»­æœç´¢ç›´åˆ°æ‰¾åˆ°çœŸå®é‚®ç®±
- è¯¦ç»†æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§
"""

import sys
import json
import time
import re
import requests
import os
import hashlib
from datetime import datetime
from urllib.parse import quote, urlencode
from bs4 import BeautifulSoup
import concurrent.futures
import logging

class SuperEmailDiscoveryEngine:
    def __init__(self):
        self.setup_logging()

        # SearxNGé…ç½® - Railwayå…¼å®¹
        self.searxng_url = os.environ.get('SEARXNG_URL', 'http://localhost:8080')

        # ç½‘ç»œä¼šè¯é…ç½® - æ— è¶…æ—¶é™åˆ¶
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': 'https://www.google.com/',
            'Connection': 'keep-alive'
        })
        # è®¾ç½®æ— é™è¶…æ—¶
        self.session.timeout = None

        # é‚®ç®±æ¨¡å¼
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')

        # ğŸ”¥ NEW: Email cache directory for deduplication across runs
        self.cache_dir = os.path.join(os.path.dirname(__file__), '.email_cache')
        os.makedirs(self.cache_dir, exist_ok=True)

        # æœç´¢çŠ¶æ€
        self.found_emails = []
        self.already_returned_emails = set()  # ğŸ”¥ NEW: Track already-returned emails
        self.search_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'emails_found': 0,
            'websites_scraped': 0,
            'unique_domains': set(),
            'query_success_rate': {}
        }

        self.logger.info("ğŸš€ è¶…çº§é‚®ç®±æœç´¢å¼•æ“åˆå§‹åŒ–")
        self.logger.info("   ğŸ“Š åŸºäº2024å¹´æœ€ä½³é‚®ç®±å‘ç°å®è·µ")
        self.logger.info("   ğŸ¯ ç›®æ ‡ï¼šç¡®ä¿æ‰¾åˆ°çœŸå®æœ‰æ•ˆçš„é‚®ç®±åœ°å€")
        self.logger.info("   ğŸ—‚ï¸ ç¼“å­˜ç›®å½•: " + self.cache_dir)
        
    def setup_logging(self):
        """è®¾ç½®è¯¦ç»†æ—¥å¿—"""
        self.logger = logging.getLogger('SuperEmailEngine')
        self.logger.setLevel(logging.INFO)

        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        file_handler = logging.FileHandler('super_email_discovery.log', encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )

        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)

        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)

    def get_cache_filename(self, industry, session_id=None):
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶åï¼ˆåŸºäºè¡Œä¸šåç§°å’Œsession IDçš„hashï¼‰"""
        # Create a hash of the industry to use as filename
        industry_hash = hashlib.md5(industry.lower().strip().encode()).hexdigest()[:12]

        # ğŸ”¥ FIX: Use session_id if provided to create campaign-specific cache
        if session_id:
            session_hash = hashlib.md5(str(session_id).encode()).hexdigest()[:8]
            return os.path.join(self.cache_dir, f'returned_emails_{industry_hash}_{session_hash}.txt')
        else:
            return os.path.join(self.cache_dir, f'returned_emails_{industry_hash}.txt')

    def load_returned_emails_cache(self, industry, session_id=None):
        """åŠ è½½å·²è¿”å›çš„é‚®ç®±ç¼“å­˜"""
        cache_file = self.get_cache_filename(industry, session_id)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_emails = {line.strip() for line in f if line.strip()}
                    self.already_returned_emails = cached_emails
                    session_info = f" (Session: {session_id})" if session_id else ""
                    self.logger.info(f"ğŸ“‚ åŠ è½½ç¼“å­˜: {len(cached_emails)} ä¸ªå·²è¿”å›é‚®ç®± (è¡Œä¸š: {industry}{session_info})")
                    return len(cached_emails)
            except Exception as e:
                self.logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
                self.already_returned_emails = set()
        else:
            self.logger.info(f"ğŸ“‚ æ— ç¼“å­˜æ–‡ä»¶ï¼Œå°†è¿”å›å…¨æ–°é‚®ç®±")
            self.already_returned_emails = set()
        return 0

    def save_returned_emails_cache(self, industry, new_emails, session_id=None):
        """ä¿å­˜æ–°è¿”å›çš„é‚®ç®±åˆ°ç¼“å­˜"""
        cache_file = self.get_cache_filename(industry, session_id)
        try:
            # Append new emails to cache file
            with open(cache_file, 'a', encoding='utf-8') as f:
                for email in new_emails:
                    f.write(email + '\n')
                    self.already_returned_emails.add(email)
            self.logger.info(f"ğŸ’¾ ç¼“å­˜å·²æ›´æ–°: +{len(new_emails)} ä¸ªé‚®ç®±")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def generate_professional_search_strategies(self, industry, round_num=1):
        """ç”ŸæˆåŸºäº2024å¹´æœ€ä½³å®è·µçš„ä¸“ä¸šæœç´¢ç­–ç•¥"""
        self.logger.info(f"ğŸ§  ç”Ÿæˆç¬¬{round_num}è½®ä¸“ä¸šæœç´¢ç­–ç•¥ - {industry}")
        
        # åŸºäºç ”ç©¶çš„æœ€æœ‰æ•ˆæœç´¢ç­–ç•¥
        base_strategies = []
        
        if round_num == 1:
            # ç¬¬ä¸€è½®ï¼šç®€çŸ­é«˜æ•ˆæœç´¢æ¨¡å¼
            base_strategies = [
                f'{industry} email contact',
                f'{industry} CEO email',
                f'{industry} founder contact',
                f'{industry} business email',
                f'{industry} company contact'
            ]
        elif round_num == 2:
            # ç¬¬äºŒè½®ï¼šç®€çŸ­å˜ä½“æœç´¢
            base_strategies = [
                f'{industry} team email',
                f'{industry} sales contact',
                f'{industry} support email',
                f'{industry} info contact',
                f'{industry} director email'
            ]
        elif round_num == 3:
            # ç¬¬ä¸‰è½®ï¼šèŒä½ç›¸å…³æœç´¢
            base_strategies = [
                f'{industry} manager email',
                f'{industry} consultant contact',
                f'{industry} specialist email',
                f'{industry} expert contact',
                f'{industry} advisor email'
            ]
        elif round_num == 4:
            # ç¬¬å››è½®ï¼šåˆ›ä¸šä¸ä¼ä¸šæœç´¢
            base_strategies = [
                f'{industry} startup email',
                f'{industry} entrepreneur contact',
                f'{industry} business owner email',
                f'{industry} partner contact',
                f'{industry} investor email'
            ]
        elif round_num == 5:
            # ç¬¬äº”è½®ï¼šéƒ¨é—¨ä¸èŒèƒ½æœç´¢
            base_strategies = [
                f'{industry} marketing email',
                f'{industry} operations contact',
                f'{industry} product manager email',
                f'{industry} customer success contact',
                f'{industry} growth email'
            ]
        elif round_num % 3 == 0:
            # æ¯3è½®ï¼šåœ°åŸŸä¸å¸‚åœºæœç´¢
            base_strategies = [
                f'{industry} North America email',
                f'{industry} Europe contact',
                f'{industry} Asia Pacific email',
                f'{industry} global contact',
                f'{industry} international email'
            ]
        elif round_num % 3 == 1:
            # æ¯3è½®+1ï¼šæŠ€æœ¯ä¸ä¸“ä¸šæœç´¢
            base_strategies = [
                f'{industry} CTO email',
                f'{industry} developer contact',
                f'{industry} engineer email',
                f'{industry} architect contact',
                f'{industry} technical lead email'
            ]
        else:
            # å…¶ä»–è½®æ¬¡ï¼šæ··åˆæœç´¢
            base_strategies = [
                f'{industry} company email',
                f'{industry} business contact',
                f'{industry} executive email',
                f'{industry} leadership contact',
                f'{industry} decision maker email'
            ]
        
        self.logger.info(f"   âœ… ç”Ÿæˆ{len(base_strategies)}ä¸ªä¸“ä¸šçº§æœç´¢ç­–ç•¥")
        return base_strategies
    
    def search_with_advanced_logging(self, query, max_results=50):
        """é«˜çº§SearxNGæœç´¢ - æ— è¶…æ—¶é™åˆ¶ï¼Œå°½å¯èƒ½å¤šåœ°è·å–ç»“æœ"""
        try:
            self.logger.info(f"ğŸ” æ·±åº¦ä¸“ä¸šæœç´¢: {query[:80]}...")
            self.search_stats['total_queries'] += 1
            
            params = {
                'q': query,
                'format': 'json',
                'categories': 'general',
                'pageno': 1
            }
            
            start_time = time.time()
            # ç§»é™¤è¶…æ—¶é™åˆ¶ - è®©æœç´¢æœ‰è¶³å¤Ÿæ—¶é—´å®Œæˆ
            response = self.session.get(f"{self.searxng_url}/search", params=params)
            duration = time.time() - start_time
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    results = data.get('results', [])
                    
                    self.logger.info(f"   âœ… æœç´¢æˆåŠŸ ({duration:.1f}s): {len(results)}ä¸ªç»“æœ")
                    self.search_stats['successful_queries'] += 1
                    
                    # åˆ†æç»“æœè´¨é‡
                    email_indicators = 0
                    contact_indicators = 0
                    
                    for result in results:
                        text = f"{result.get('title', '')} {result.get('content', '')}".lower()
                        if '@' in text:
                            email_indicators += 1
                        if any(word in text for word in ['contact', 'email', 'reach']):
                            contact_indicators += 1
                    
                    self.logger.info(f"   ğŸ“Š è´¨é‡åˆ†æ: {email_indicators}ä¸ª@ç¬¦å·, {contact_indicators}ä¸ªè”ç³»æŒ‡ç¤ºå™¨")
                    
                    # è®°å½•æŸ¥è¯¢æˆåŠŸç‡
                    query_type = self.classify_query_type(query)
                    if query_type not in self.search_stats['query_success_rate']:
                        self.search_stats['query_success_rate'][query_type] = {'success': 0, 'total': 0}
                    
                    self.search_stats['query_success_rate'][query_type]['total'] += 1
                    if email_indicators > 0:
                        self.search_stats['query_success_rate'][query_type]['success'] += 1
                    
                    return results[:max_results]
                    
                except json.JSONDecodeError as e:
                    self.logger.error(f"   âŒ JSONè§£æå¤±è´¥: {str(e)}")
                    return []
            else:
                self.logger.error(f"   âŒ æœç´¢è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"   âŒ æœç´¢é”™è¯¯: {str(e)}")
            return []
    
    def classify_query_type(self, query):
        """åˆ†ç±»æœç´¢æŸ¥è¯¢ç±»å‹ä»¥è¿›è¡Œæ€§èƒ½åˆ†æ"""
        query_lower = query.lower()
        if 'site:linkedin.com' in query_lower:
            return 'linkedin_search'
        elif 'site:' in query_lower:
            return 'site_specific'
        elif 'filetype:' in query_lower:
            return 'file_search'
        elif 'intext:' in query_lower:
            return 'content_search'
        else:
            return 'general_search'
    
    def extract_emails_advanced(self, text, source=""):
        """é«˜çº§é‚®ç®±æå– - ä½¿ç”¨2024å¹´æœ€ä½³æ¨¡å¼"""
        if not text:
            return []
            
        # æ‰¾åˆ°æ‰€æœ‰æ½œåœ¨é‚®ç®±
        potential_emails = self.email_pattern.findall(text)
        
        valid_emails = []
        excluded_count = 0
        
        for email in potential_emails:
            email_lower = email.lower()
            
            # 2024å¹´æ›´æ–°çš„æ’é™¤è§„åˆ™
            exclusions = [
                'example.com', 'test.com', 'domain.com', 'yoursite.com', 'company.com',
                'noreply', 'no-reply', 'donotreply', 'bounce', 'mailer-daemon',
                'privacy@', 'legal@', 'abuse@', 'postmaster@', 'webmaster@',
                'support@example', 'admin@example', 'info@example', 'sales@example',
                'sample@', 'demo@', 'fake@', 'null@', 'void@'
            ]
            
            if any(pattern in email_lower for pattern in exclusions):
                excluded_count += 1
                continue
            
            # éªŒè¯é‚®ç®±æ ¼å¼
            if self.validate_email_format(email):
                valid_emails.append(email)
                domain = email.split('@')[1]
                self.search_stats['unique_domains'].add(domain)
                self.logger.info(f"   âœ… å‘ç°æœ‰æ•ˆé‚®ç®±: {email} (æ¥æº: {source[:30]})")
        
        if excluded_count > 0:
            self.logger.debug(f"   ğŸ—‘ï¸ æ’é™¤äº†{excluded_count}ä¸ªç¤ºä¾‹/æ— æ•ˆé‚®ç®±")
        
        return list(set(valid_emails))
    
    def validate_email_format(self, email):
        """éªŒè¯é‚®ç®±æ ¼å¼"""
        if not (5 < len(email) < 100 and email.count('@') == 1):
            return False
        
        local, domain = email.split('@')
        
        # æ£€æŸ¥æœ¬åœ°éƒ¨åˆ†
        if not local or len(local) > 64:
            return False
        
        # æ£€æŸ¥åŸŸåéƒ¨åˆ†
        if not domain or '.' not in domain or len(domain) < 4:
            return False
        
        # æ£€æŸ¥é¡¶çº§åŸŸå
        tld = domain.split('.')[-1]
        if len(tld) < 2 or not tld.isalpha():
            return False
        
        return True
    
    def scrape_website_advanced(self, url):
        """é«˜çº§ç½‘ç«™çˆ¬å– - ä¸“æ³¨è”ç³»ä¿¡æ¯ï¼Œæ— æ—¶é—´é™åˆ¶"""
        try:
            self.logger.info(f"   ğŸŒ æ·±åº¦æ— é™çˆ¬å–: {url[:60]}...")
            self.search_stats['websites_scraped'] += 1
            
            start_time = time.time()
            # ç§»é™¤è¶…æ—¶é™åˆ¶ - è®©çˆ¬å–æœ‰å……è¶³æ—¶é—´
            response = self.session.get(url)
            duration = time.time() - start_time
            
            if response.status_code != 200:
                self.logger.warning(f"   âš ï¸ HTTP {response.status_code}: {url}")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤å¹²æ‰°å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()
            
            # ä¼˜å…ˆæœç´¢è”ç³»ç›¸å…³åŒºåŸŸ
            priority_areas = []
            
            # æŸ¥æ‰¾è”ç³»é¡µé¢å…³é”®åŒºåŸŸ
            contact_selectors = [
                '[class*="contact"]', '[id*="contact"]',
                '[class*="about"]', '[id*="about"]', 
                '[class*="team"]', '[id*="team"]',
                '[class*="staff"]', '[id*="staff"]',
                '[class*="press"]', '[id*="press"]',
                '[class*="media"]', '[id*="media"]'
            ]
            
            for selector in contact_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    priority_areas.append(elem.get_text())
            
            # è·å–ä¸»è¦å†…å®¹
            main_content = soup.get_text()
            
            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ï¼Œä¼˜å…ˆå¤„ç†è”ç³»åŒºåŸŸ
            all_text = ' '.join(priority_areas) + ' ' + main_content
            
            emails = self.extract_emails_advanced(all_text, f"ç½‘ç«™ {url}")
            
            self.logger.info(f"   âœ… çˆ¬å–å®Œæˆ ({duration:.1f}s): {len(emails)}ä¸ªé‚®ç®±")
            return emails
            
        except Exception as e:
            self.logger.error(f"   âŒ çˆ¬å–å¤±è´¥ {url}: {str(e)}")
            return []
    
    def execute_persistent_discovery(self, industry, target_count=5, max_rounds=None, session_id=None):
        """æ‰§è¡Œæ— é™åˆ¶æŒç»­æœç´¢ - è¶Šå¤šè¶Šå‡†ç¡®"""
        # ğŸ”¥ FIX: Scale max_rounds based on target_count
        # Each round finds ~5-15 new emails on average (after filtering cached)
        # Use at least 100 rounds, scale up for larger requests, cap at 500 for safety
        if max_rounds is None:
            max_rounds = min(500, max(100, target_count // 5))  # ~5 emails per round, max 500 rounds

        self.logger.info(f"ğŸš€ å¯åŠ¨æ— é™åˆ¶è¶…çº§é‚®ç®±æœç´¢ - {industry}")
        self.logger.info(f"   ğŸ¯ ç›®æ ‡: {target_count}ä¸ªNEWé‚®ç®± (è·³è¿‡å·²è¿”å›)")
        self.logger.info(f"   ğŸ”„ æœ€å¤§è½®æ•°: {max_rounds} (åŠ¨æ€è°ƒæ•´ï¼Œç¡®ä¿æ‰¾åˆ°è¶³å¤Ÿæ–°é‚®ç®±)")
        self.logger.info(f"   ğŸ“Š ä½¿ç”¨2024å¹´æœ€ä½³æœç´¢å®è·µ")
        self.logger.info(f"   â° æ— æ—¶é—´é™åˆ¶ - æŒç»­æœç´¢ç›´åˆ°æ‰¾åˆ°è¶³å¤Ÿæ–°é‚®ç®±")
        if session_id:
            self.logger.info(f"   ğŸ”‘ Session ID: {session_id} (campaign-specific cache)")

        # ğŸ”¥ FIX: Load cache of already-returned emails with session_id
        cached_count = self.load_returned_emails_cache(industry, session_id)
        if cached_count > 0:
            self.logger.info(f"   ğŸ”„ è·³è¿‡å·²è¿”å›çš„ {cached_count} ä¸ªé‚®ç®±ï¼Œå¯»æ‰¾æ–°é‚®ç®±...")

        start_time = time.time()
        all_emails = []
        round_num = 1
        consecutive_empty_rounds = 0
        total_emails_found = 0  # ğŸ”¥ FIX: Track total including duplicates
        total_cached_skipped = 0  # ğŸ”¥ FIX: Track how many cached emails skipped
        
        while len(all_emails) < target_count and round_num <= max_rounds:
            self.logger.info(f"\nğŸ“ ç¬¬{round_num}è½®æœç´¢ (å·²æ‰¾åˆ° {len(all_emails)}/{target_count})")
            
            # ç”Ÿæˆæœ¬è½®ç­–ç•¥
            strategies = self.generate_professional_search_strategies(industry, round_num)
            round_emails = []
            
            for i, strategy in enumerate(strategies, 1):
                self.logger.info(f"   ğŸ¯ ç­–ç•¥{i}/{len(strategies)}: {strategy[:70]}...")
                
                
                # æœç´¢
                results = self.search_with_advanced_logging(strategy)
                
                if not results:
                    self.logger.warning(f"   âš ï¸ ç­–ç•¥{i} æ— ç»“æœ")
                    continue
                
                # ä»æœç´¢é¢„è§ˆæå–é‚®ç®±
                preview_emails = []
                for result in results:
                    text = f"{result.get('title', '')} {result.get('content', '')}"
                    emails = self.extract_emails_advanced(text, f"æœç´¢é¢„è§ˆ {i}")

                    for email in emails:
                        total_emails_found += 1  # ğŸ”¥ FIX: Count all emails found
                        # ğŸ”¥ NEW: Skip already-returned emails
                        if email in self.already_returned_emails:
                            total_cached_skipped += 1  # ğŸ”¥ FIX: Track skipped
                            continue
                        if not any(e['email'] == email for e in preview_emails):
                            preview_emails.append({
                                'email': email,
                                'source': 'search_preview',
                                'source_url': result.get('url', ''),
                                'source_title': result.get('title', ''),
                                'confidence': 0.8,
                                'round': round_num,
                                'strategy': strategy,
                                'discovery_method': 'professional_search'
                            })
                
                round_emails.extend(preview_emails)
                self.logger.info(f"   ğŸ“§ ç­–ç•¥{i}é¢„è§ˆ: {len(preview_emails)}ä¸ªé‚®ç®±")
                
                # å¹¶è¡Œçˆ¬å–æ›´å¤šç½‘ç«™ - æ— é™åˆ¶æ¨¡å¼
                promising_sites = [r for r in results[:20] 
                                 if any(word in r.get('url', '').lower() 
                                       for word in ['contact', 'about', 'team', 'press'])]
                
                if not promising_sites:
                    promising_sites = results[:15]  # å¢åŠ å¤‡é€‰æ–¹æ¡ˆæ•°é‡
                
                self.logger.info(f"   ğŸŒ æ·±åº¦å¹¶è¡Œçˆ¬å–{len(promising_sites)}ä¸ªç½‘ç«™ (æ— æ—¶é—´é™åˆ¶)...")
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
                    future_to_result = {
                        executor.submit(self.scrape_website_advanced, site['url']): site 
                        for site in promising_sites
                    }
                    
                    # ç§»é™¤è¶…æ—¶é™åˆ¶ï¼Œè®©æ‰€æœ‰ç½‘ç«™éƒ½æœ‰å……è¶³æ—¶é—´å®Œæˆ
                    for future in concurrent.futures.as_completed(future_to_result):
                        try:
                            site = future_to_result[future]
                            website_emails = future.result()

                            for email in website_emails:
                                total_emails_found += 1  # ğŸ”¥ FIX: Count all emails found
                                # ğŸ”¥ NEW: Skip already-returned emails
                                if email in self.already_returned_emails:
                                    total_cached_skipped += 1  # ğŸ”¥ FIX: Track skipped
                                    continue
                                if not any(e['email'] == email for e in round_emails):
                                    round_emails.append({
                                        'email': email,
                                        'source': 'website_scraping',
                                        'source_url': site['url'],
                                        'source_title': site.get('title', ''),
                                        'confidence': 0.95,
                                        'round': round_num,
                                        'strategy': strategy,
                                        'discovery_method': 'deep_scraping'
                                    })
                        except Exception as e:
                            continue
                
                # æ£€æŸ¥è¿›åº¦ï¼Œä½†ä¸ç«‹å³åœæ­¢ - è®©å®ƒç»§ç»­æœç´¢æ›´å¤š
                all_unique = {e['email']: e for e in all_emails + round_emails}
                if len(all_unique) >= target_count:
                    self.logger.info(f"ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡ï¼Œä½†ç»§ç»­æœç´¢ä»¥è·å¾—æ›´å‡†ç¡®ç»“æœ...")
                    # ä¸breakï¼Œç»§ç»­æœç´¢
                
                time.sleep(0.3)  # å‡å°‘ç­–ç•¥é—´éš”
            
            # æ›´æ–°æ€»é‚®ç®±åˆ—è¡¨
            all_emails.extend(round_emails)
            all_unique = {e['email']: e for e in all_emails}
            all_emails = list(all_unique.values())

            # ğŸ”¥ FIX: Show detailed statistics including cached skips
            self.logger.info(f"ğŸ“Š ç¬¬{round_num}è½®ç»“æœ: æ–°å¢{len(round_emails)}ä¸ªï¼Œæ€»è®¡{len(all_emails)}ä¸ªNEWé‚®ç®±")
            if total_cached_skipped > 0:
                self.logger.info(f"   ğŸ”„ å·²è·³è¿‡ {total_cached_skipped} ä¸ªé‡å¤/ç¼“å­˜é‚®ç®± (æ€»å‘ç°{total_emails_found}ä¸ª)")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´ç­–ç•¥ï¼Œä½†ä¸è½»æ˜“æ”¾å¼ƒ
            if len(round_emails) == 0:
                consecutive_empty_rounds += 1
                self.logger.warning(f"âš ï¸ è¿ç»­{consecutive_empty_rounds}è½®æ— ç»“æœ - ç»§ç»­å°è¯•")
                
                if consecutive_empty_rounds >= 5:  # å¢åŠ å®¹å¿åº¦
                    self.logger.info("ğŸ”„ åˆ‡æ¢åˆ°æ›´å¹¿æ³›çš„æœç´¢ç­–ç•¥...")
            else:
                consecutive_empty_rounds = 0
            
            # å³ä½¿è¾¾åˆ°ç›®æ ‡ä¹Ÿä¸ç«‹å³é€€å‡º - ç»§ç»­æœç´¢è·å¾—æ›´å¤šé‚®ç®±
            if len(all_emails) >= target_count and round_num >= 5:
                self.logger.info(f"ğŸ¯ å·²æ”¶é›†è¶³å¤Ÿé‚®ç®±å¹¶è¿›è¡Œäº†å……åˆ†æœç´¢ï¼Œå‡†å¤‡ç»“æŸ")
                break
            
            round_num += 1
            if round_num <= max_rounds:
                time.sleep(1)  # å‡å°‘è½®æ¬¡é—´éš”
        
        # æ•´ç†æœ€ç»ˆç»“æœ
        final_emails = all_emails[:target_count]
        total_time = time.time() - start_time

        # æ›´æ–°ç»Ÿè®¡
        self.search_stats['emails_found'] = len(final_emails)

        # ğŸ”¥ FIX: Save newly returned emails to cache with session_id
        new_email_addresses = [e['email'] for e in final_emails]
        if new_email_addresses:
            self.save_returned_emails_cache(industry, new_email_addresses, session_id)
            self.logger.info(f"   âœ… å·²ä¿å­˜ {len(new_email_addresses)} ä¸ªæ–°é‚®ç®±åˆ°ç¼“å­˜")

        self.logger.info(f"\nğŸŠ è¶…çº§æœç´¢å®Œæˆï¼")
        self.logger.info(f"   ğŸ“§ æœ€ç»ˆé‚®ç®±: {len(final_emails)}ä¸ªNEWé‚®ç®± (å…¨éƒ¨ä¸ºæ–°å‘ç°)")
        self.logger.info(f"   ğŸ”„ æœç´¢è½®æ•°: {round_num-1}")
        self.logger.info(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’")
        self.logger.info(f"   ğŸ“Š æˆåŠŸç‡: {self.search_stats['successful_queries']}/{self.search_stats['total_queries']}")
        self.logger.info(f"   ğŸŒ çˆ¬å–ç½‘ç«™: {self.search_stats['websites_scraped']}ä¸ª")
        self.logger.info(f"   ğŸ¢ å‘ç°åŸŸå: {len(self.search_stats['unique_domains'])}ä¸ª")
        self.logger.info(f"   ğŸ”„ æ€»å‘ç°: {total_emails_found}ä¸ª (è·³è¿‡{total_cached_skipped}ä¸ªé‡å¤)")
        self.logger.info(f"   ğŸ—‚ï¸ ç¼“å­˜æ€»æ•°: {len(self.already_returned_emails)} ä¸ªå†å²é‚®ç®±")

        # æ˜¾ç¤ºå‘ç°çš„é‚®ç®±
        if final_emails:
            self.logger.info("\nğŸ“§ å‘ç°çš„é‚®ç®±åœ°å€ (æ–°):")
            for i, email_data in enumerate(final_emails, 1):
                self.logger.info(f"   {i}. {email_data['email']} (ç½®ä¿¡åº¦: {email_data['confidence']})")

        return {
            'success': len(final_emails) > 0,
            'emails': [e['email'] for e in final_emails],
            'email_details': final_emails,
            'total_emails': len(final_emails),
            'search_rounds': round_num - 1,
            'execution_time': total_time,
            'search_stats': self.prepare_stats_for_json(),
            'industry': industry,
            'target_achieved': len(final_emails) >= target_count,
            'method': 'super_email_discovery_2024',
            'confidence_score': sum(e['confidence'] for e in final_emails) / len(final_emails) if final_emails else 0,
            'timestamp': datetime.now().isoformat()
        }
    
    def prepare_stats_for_json(self):
        """å‡†å¤‡ç»Ÿè®¡æ•°æ®ç”¨äºJSONåºåˆ—åŒ–"""
        stats = dict(self.search_stats)
        stats['unique_domains'] = list(self.search_stats['unique_domains'])
        return stats

def main():
    if len(sys.argv) < 2:
        print('ä½¿ç”¨æ–¹æ³•: python3 SuperEmailDiscoveryEngine.py "è¡Œä¸šåç§°" [é‚®ç®±æ•°é‡] [session_id]')
        print('ç¤ºä¾‹: python3 SuperEmailDiscoveryEngine.py "AI startup" 5 campaign_123')
        return

    industry = sys.argv[1]
    target_count = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    session_id = sys.argv[3] if len(sys.argv) > 3 else None  # ğŸ”¥ FIX: Accept session_id

    engine = SuperEmailDiscoveryEngine()
    # ğŸ”¥ FIX: Let max_rounds be calculated dynamically based on target_count
    results = engine.execute_persistent_discovery(industry, target_count, session_id=session_id)
    
    print("\n" + "="*90)
    print("ğŸ¯ è¶…çº§é‚®ç®±æœç´¢å¼•æ“ - æœ€ç»ˆæŠ¥å‘Š")
    print("="*90)
    
    if results['success']:
        print(f"âœ… æˆåŠŸå‘ç° {results['total_emails']} ä¸ªé‚®ç®±åœ°å€:")
        for i, email in enumerate(results['emails'], 1):
            print(f"   {i}. {email}")
        
        print(f"\nğŸ“Š æœç´¢æ€§èƒ½æŒ‡æ ‡:")
        print(f"   ğŸ”„ æœç´¢è½®æ•°: {results['search_rounds']}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {results['execution_time']:.1f}ç§’")
        print(f"   ğŸ¯ ç›®æ ‡è¾¾æˆ: {'æ˜¯' if results['target_achieved'] else 'å¦'}")
        print(f"   ğŸ“ˆ æŸ¥è¯¢æˆåŠŸç‡: {results['search_stats']['successful_queries']}/{results['search_stats']['total_queries']}")
        print(f"   ğŸŒ ç½‘ç«™çˆ¬å–: {results['search_stats']['websites_scraped']}ä¸ª")
        print(f"   ğŸ¢ å‘ç°åŸŸå: {len(results['search_stats']['unique_domains'])}ä¸ª")
        print(f"   ğŸ­ å¹³å‡ç½®ä¿¡åº¦: {results['confidence_score']:.2f}")
        
    else:
        print("âŒ æœªèƒ½å‘ç°é‚®ç®±åœ°å€")
        print("ğŸ’¡ å»ºè®®ï¼šå°è¯•æ›´å…·ä½“çš„è¡Œä¸šæè¿°æˆ–å¢åŠ æœç´¢è½®æ•°")
    
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ (JSON):")
    print(json.dumps(results, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()