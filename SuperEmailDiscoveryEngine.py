#!/usr/bin/env python3
"""
è¶…çº§é‚®ç®±æœç´¢å¼•æ“ - åŸºäº2024å¹´æœ€ä½³å®è·µ
- ä½¿ç”¨æœ€æœ‰æ•ˆçš„Googleæœç´¢æ“ä½œç¬¦
- åŸºäºä¸“ä¸šé‚®ç®±å‘ç°å·¥å…·çš„ç­–ç•¥
- æŒç»­æœç´¢ç›´åˆ°æ‰¾åˆ°çœŸå®é‚®ç®±
- è¯¦ç»†æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§
"""

import sys
import json
import time
import re
import requests
import os
import hashlib
import dns.resolver
from datetime import datetime
from urllib.parse import quote, urlencode
from bs4 import BeautifulSoup
import concurrent.futures
import logging

class SuperEmailDiscoveryEngine:
    def __init__(self):
        self.setup_logging()

        # SearxNGé…ç½® - Railwayå…¼å®¹
        self.searxng_url = os.environ.get('SEARXNG_URL', 'http://localhost:8080')

        # ç½‘ç»œä¼šè¯é…ç½® - æ— è¶…æ—¶é™åˆ¶
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': 'https://www.google.com/',
            'Connection': 'keep-alive'
        })
        # è®¾ç½®æ— é™è¶…æ—¶
        self.session.timeout = None

        # é‚®ç®±æ¨¡å¼
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')

        # ğŸ”¥ NEW: Email cache directory for deduplication across runs
        self.cache_dir = os.path.join(os.path.dirname(__file__), '.email_cache')
        os.makedirs(self.cache_dir, exist_ok=True)

        # æœç´¢çŠ¶æ€
        self.found_emails = []
        self.already_returned_emails = set()  # ğŸ”¥ NEW: Track already-returned emails
        self.search_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'emails_found': 0,
            'websites_scraped': 0,
            'unique_domains': set(),
            'query_success_rate': {}
        }

        self.logger.info("ğŸš€ è¶…çº§é‚®ç®±æœç´¢å¼•æ“åˆå§‹åŒ–")
        self.logger.info("   ğŸ“Š åŸºäº2024å¹´æœ€ä½³é‚®ç®±å‘ç°å®è·µ")
        self.logger.info("   ğŸ¯ ç›®æ ‡ï¼šç¡®ä¿æ‰¾åˆ°çœŸå®æœ‰æ•ˆçš„é‚®ç®±åœ°å€")
        self.logger.info("   ğŸ—‚ï¸ ç¼“å­˜ç›®å½•: " + self.cache_dir)
        
    def setup_logging(self):
        """è®¾ç½®è¯¦ç»†æ—¥å¿—"""
        self.logger = logging.getLogger('SuperEmailEngine')
        self.logger.setLevel(logging.INFO)

        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        file_handler = logging.FileHandler('super_email_discovery.log', encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )

        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)

        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)

    def get_cache_filename(self, industry, session_id=None):
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶åï¼ˆåŸºäºè¡Œä¸šåç§°å’Œsession IDçš„hashï¼‰"""
        # Create a hash of the industry to use as filename
        industry_hash = hashlib.md5(industry.lower().strip().encode()).hexdigest()[:12]

        # ğŸ”¥ FIX: Use session_id if provided to create campaign-specific cache
        if session_id:
            session_hash = hashlib.md5(str(session_id).encode()).hexdigest()[:8]
            return os.path.join(self.cache_dir, f'returned_emails_{industry_hash}_{session_hash}.txt')
        else:
            return os.path.join(self.cache_dir, f'returned_emails_{industry_hash}.txt')

    def load_returned_emails_cache(self, industry, session_id=None):
        """åŠ è½½å·²è¿”å›çš„é‚®ç®±ç¼“å­˜"""
        cache_file = self.get_cache_filename(industry, session_id)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_emails = {line.strip() for line in f if line.strip()}
                    self.already_returned_emails = cached_emails
                    session_info = f" (Session: {session_id})" if session_id else ""
                    self.logger.info(f"ğŸ“‚ åŠ è½½ç¼“å­˜: {len(cached_emails)} ä¸ªå·²è¿”å›é‚®ç®± (è¡Œä¸š: {industry}{session_info})")
                    return len(cached_emails)
            except Exception as e:
                self.logger.warning(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
                self.already_returned_emails = set()
        else:
            self.logger.info(f"ğŸ“‚ æ— ç¼“å­˜æ–‡ä»¶ï¼Œå°†è¿”å›å…¨æ–°é‚®ç®±")
            self.already_returned_emails = set()
        return 0

    def save_returned_emails_cache(self, industry, new_emails, session_id=None):
        """ä¿å­˜æ–°è¿”å›çš„é‚®ç®±åˆ°ç¼“å­˜"""
        cache_file = self.get_cache_filename(industry, session_id)
        try:
            # Append new emails to cache file
            with open(cache_file, 'a', encoding='utf-8') as f:
                for email in new_emails:
                    f.write(email + '\n')
                    self.already_returned_emails.add(email)
            self.logger.info(f"ğŸ’¾ ç¼“å­˜å·²æ›´æ–°: +{len(new_emails)} ä¸ªé‚®ç®±")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def generate_professional_search_strategies(self, industry, round_num=1):
        """ç”ŸæˆåŸºäº2024å¹´æœ€ä½³å®è·µçš„ä¸“ä¸šæœç´¢ç­–ç•¥"""
        self.logger.info(f"ğŸ§  ç”Ÿæˆç¬¬{round_num}è½®ä¸“ä¸šæœç´¢ç­–ç•¥ - {industry}")
        
        # åŸºäºç ”ç©¶çš„æœ€æœ‰æ•ˆæœç´¢ç­–ç•¥
        base_strategies = []
        
        if round_num == 1:
            # ç¬¬ä¸€è½®ï¼šç®€çŸ­é«˜æ•ˆæœç´¢æ¨¡å¼
            base_strategies = [
                f'{industry} email contact',
                f'{industry} CEO email',
                f'{industry} founder contact',
                f'{industry} business email',
                f'{industry} company contact'
            ]
        elif round_num == 2:
            # ç¬¬äºŒè½®ï¼šç®€çŸ­å˜ä½“æœç´¢
            base_strategies = [
                f'{industry} team email',
                f'{industry} sales contact',
                f'{industry} support email',
                f'{industry} info contact',
                f'{industry} director email'
            ]
        elif round_num == 3:
            # ç¬¬ä¸‰è½®ï¼šèŒä½ç›¸å…³æœç´¢
            base_strategies = [
                f'{industry} manager email',
                f'{industry} consultant contact',
                f'{industry} specialist email',
                f'{industry} expert contact',
                f'{industry} advisor email'
            ]
        elif round_num == 4:
            # ç¬¬å››è½®ï¼šåˆ›ä¸šä¸ä¼ä¸šæœç´¢
            base_strategies = [
                f'{industry} startup email',
                f'{industry} entrepreneur contact',
                f'{industry} business owner email',
                f'{industry} partner contact',
                f'{industry} investor email'
            ]
        elif round_num == 5:
            # ç¬¬äº”è½®ï¼šéƒ¨é—¨ä¸èŒèƒ½æœç´¢
            base_strategies = [
                f'{industry} marketing email',
                f'{industry} operations contact',
                f'{industry} product manager email',
                f'{industry} customer success contact',
                f'{industry} growth email'
            ]
        elif round_num % 3 == 0:
            # æ¯3è½®ï¼šåœ°åŸŸä¸å¸‚åœºæœç´¢
            base_strategies = [
                f'{industry} North America email',
                f'{industry} Europe contact',
                f'{industry} Asia Pacific email',
                f'{industry} global contact',
                f'{industry} international email'
            ]
        elif round_num % 3 == 1:
            # æ¯3è½®+1ï¼šæŠ€æœ¯ä¸ä¸“ä¸šæœç´¢
            base_strategies = [
                f'{industry} CTO email',
                f'{industry} developer contact',
                f'{industry} engineer email',
                f'{industry} architect contact',
                f'{industry} technical lead email'
            ]
        else:
            # å…¶ä»–è½®æ¬¡ï¼šæ··åˆæœç´¢
            base_strategies = [
                f'{industry} company email',
                f'{industry} business contact',
                f'{industry} executive email',
                f'{industry} leadership contact',
                f'{industry} decision maker email'
            ]
        
        self.logger.info(f"   âœ… ç”Ÿæˆ{len(base_strategies)}ä¸ªä¸“ä¸šçº§æœç´¢ç­–ç•¥")
        return base_strategies
    
    def search_with_advanced_logging(self, query, max_results=50):
        """é«˜çº§SearxNGæœç´¢ - æ— è¶…æ—¶é™åˆ¶ï¼Œå°½å¯èƒ½å¤šåœ°è·å–ç»“æœ"""
        try:
            self.logger.info(f"ğŸ” æ·±åº¦ä¸“ä¸šæœç´¢: {query[:80]}...")
            self.search_stats['total_queries'] += 1
            
            params = {
                'q': query,
                'format': 'json',
                'categories': 'general',
                'pageno': 1
            }
            
            start_time = time.time()
            # ç§»é™¤è¶…æ—¶é™åˆ¶ - è®©æœç´¢æœ‰è¶³å¤Ÿæ—¶é—´å®Œæˆ
            response = self.session.get(f"{self.searxng_url}/search", params=params)
            duration = time.time() - start_time
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    results = data.get('results', [])
                    
                    self.logger.info(f"   âœ… æœç´¢æˆåŠŸ ({duration:.1f}s): {len(results)}ä¸ªç»“æœ")
                    self.search_stats['successful_queries'] += 1
                    
                    # åˆ†æç»“æœè´¨é‡
                    email_indicators = 0
                    contact_indicators = 0
                    
                    for result in results:
                        text = f"{result.get('title', '')} {result.get('content', '')}".lower()
                        if '@' in text:
                            email_indicators += 1
                        if any(word in text for word in ['contact', 'email', 'reach']):
                            contact_indicators += 1
                    
                    self.logger.info(f"   ğŸ“Š è´¨é‡åˆ†æ: {email_indicators}ä¸ª@ç¬¦å·, {contact_indicators}ä¸ªè”ç³»æŒ‡ç¤ºå™¨")
                    
                    # è®°å½•æŸ¥è¯¢æˆåŠŸç‡
                    query_type = self.classify_query_type(query)
                    if query_type not in self.search_stats['query_success_rate']:
                        self.search_stats['query_success_rate'][query_type] = {'success': 0, 'total': 0}
                    
                    self.search_stats['query_success_rate'][query_type]['total'] += 1
                    if email_indicators > 0:
                        self.search_stats['query_success_rate'][query_type]['success'] += 1
                    
                    return results[:max_results]
                    
                except json.JSONDecodeError as e:
                    self.logger.error(f"   âŒ JSONè§£æå¤±è´¥: {str(e)}")
                    return []
            else:
                self.logger.error(f"   âŒ æœç´¢è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"   âŒ æœç´¢é”™è¯¯: {str(e)}")
            return []
    
    def classify_query_type(self, query):
        """åˆ†ç±»æœç´¢æŸ¥è¯¢ç±»å‹ä»¥è¿›è¡Œæ€§èƒ½åˆ†æ"""
        query_lower = query.lower()
        if 'site:linkedin.com' in query_lower:
            return 'linkedin_search'
        elif 'site:' in query_lower:
            return 'site_specific'
        elif 'filetype:' in query_lower:
            return 'file_search'
        elif 'intext:' in query_lower:
            return 'content_search'
        else:
            return 'general_search'
    
    def is_personal_email(self, email):
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸ªäººé‚®ç®±ï¼ˆéé€šç”¨é‚®ç®±ï¼‰"""
        generic_prefixes = [
            'info', 'contact', 'hello', 'hi', 'support', 'help', 'admin',
            'sales', 'marketing', 'office', 'general', 'inquiry', 'service',
            'careers', 'jobs', 'hr', 'feedback', 'team', 'press', 'media',
            'noreply', 'no-reply', 'webmaster', 'postmaster'
        ]

        username = email.split('@')[0].lower()

        # é€šç”¨é‚®ç®±åˆ¤æ–­
        if any(username.startswith(prefix) for prefix in generic_prefixes):
            return False
        if any(username == prefix for prefix in generic_prefixes):
            return False

        # ä¸ªäººé‚®ç®±é€šå¸¸åŒ…å«åå­—ï¼ˆæœ‰ç‚¹ã€ä¸‹åˆ’çº¿æˆ–é©¼å³°å‘½åï¼‰
        if '.' in username or '_' in username:
            return True
        if any(c.isupper() for c in email.split('@')[0]):  # é©¼å³°å‘½å
            return True

        # åå­—é•¿åº¦åˆ¤æ–­ï¼ˆä¸ªäººé‚®ç®±é€šå¸¸5-20å­—ç¬¦ï¼‰
        if 5 <= len(username) <= 20 and username.isalpha():
            return True

        return False

    def extract_context_around_email(self, html_content, email):
        """æå–é‚®ç®±å‘¨å›´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå§“åã€èŒä½ã€éƒ¨é—¨ï¼‰"""
        if not html_content or not email:
            return {}

        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')

            # æŸ¥æ‰¾åŒ…å«æ­¤é‚®ç®±çš„å…ƒç´ 
            email_elements = soup.find_all(string=re.compile(re.escape(email)))

            context = {
                'name': None,
                'title': None,
                'department': None
            }

            # èŒä½å…³é”®è¯
            title_keywords = [
                'CEO', 'CTO', 'CFO', 'COO', 'President', 'Vice President', 'VP',
                'Director', 'Manager', 'Head', 'Lead', 'Chief', 'Founder',
                'Engineer', 'Developer', 'Scientist', 'Researcher', 'Analyst',
                'Coordinator', 'Specialist', 'Consultant', 'Advisor'
            ]

            # éƒ¨é—¨å…³é”®è¯
            dept_keywords = [
                'Engineering', 'Marketing', 'Sales', 'Finance', 'HR',
                'Human Resources', 'Operations', 'IT', 'Technology', 'Product',
                'Research', 'Development', 'Customer Success', 'Support',
                'Food Science', 'Nutrition', 'Culinary', 'Agriculture'
            ]

            for elem in email_elements:
                parent = elem.parent
                if not parent:
                    continue

                # è·å–çˆ¶å…ƒç´ åŠå…¶å‘¨å›´çš„æ–‡æœ¬
                context_text = parent.get_text(separator=' ', strip=True)

                # æ‰©å±•åˆ°æ›´å¤§çš„ä¸Šä¸‹æ–‡ï¼ˆç¥–çˆ¶å…ƒç´ ï¼‰
                if parent.parent:
                    context_text += ' ' + parent.parent.get_text(separator=' ', strip=True)

                # æå–èŒä½
                for title_kw in title_keywords:
                    if title_kw.lower() in context_text.lower():
                        context['title'] = title_kw
                        break

                # æå–éƒ¨é—¨
                for dept_kw in dept_keywords:
                    if dept_kw.lower() in context_text.lower():
                        context['department'] = dept_kw
                        break

                # å°è¯•æå–å§“åï¼ˆé‚®ç®±é™„è¿‘çš„å¤§å†™å•è¯æ¨¡å¼ï¼‰
                # åŒ¹é… "John Smith" æˆ– "Dr. John Smith" ç­‰æ¨¡å¼
                name_pattern = r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)\b'
                names = re.findall(name_pattern, context_text)
                if names and not context['name']:
                    # è¿‡æ»¤æ‰å…¬å¸åã€èŒä½åç­‰
                    for name in names:
                        name_lower = name.lower()
                        # æ’é™¤èŒä½å…³é”®è¯
                        if not any(kw.lower() in name_lower for kw in title_keywords):
                            # æ’é™¤éƒ¨é—¨å…³é”®è¯
                            if not any(kw.lower() in name_lower for kw in dept_keywords):
                                context['name'] = name
                                break

                # å¦‚æœæ‰¾åˆ°äº†æœ‰ç”¨ä¿¡æ¯ï¼Œæå‰ç»“æŸ
                if context['name'] or context['title'] or context['department']:
                    break

            return context

        except Exception as e:
            self.logger.debug(f"   âš ï¸ ä¸Šä¸‹æ–‡æå–å¤±è´¥: {e}")
            return {}

    def extract_emails_advanced(self, text, source="", html_content=None):
        """é«˜çº§é‚®ç®±æå– - ä½¿ç”¨2024å¹´æœ€ä½³æ¨¡å¼ + ä¼˜å…ˆä¸ªäººé‚®ç®±"""
        if not text:
            return []

        # æ‰¾åˆ°æ‰€æœ‰æ½œåœ¨é‚®ç®±
        potential_emails = self.email_pattern.findall(text)

        valid_emails = []
        excluded_count = 0

        for email in potential_emails:
            email_lower = email.lower()

            # 2024å¹´æ›´æ–°çš„æ’é™¤è§„åˆ™
            exclusions = [
                'example.com', 'test.com', 'domain.com', 'yoursite.com', 'company.com',
                'noreply', 'no-reply', 'donotreply', 'bounce', 'mailer-daemon',
                'privacy@', 'legal@', 'abuse@', 'postmaster@', 'webmaster@',
                'support@example', 'admin@example', 'info@example', 'sales@example',
                'sample@', 'demo@', 'fake@', 'null@', 'void@'
            ]

            if any(pattern in email_lower for pattern in exclusions):
                excluded_count += 1
                continue

            # éªŒè¯é‚®ç®±æ ¼å¼
            if self.validate_email_format(email):
                # æå–é‚®ç®±å‘¨å›´çš„ä¸Šä¸‹æ–‡ï¼ˆå§“åã€èŒä½ã€éƒ¨é—¨ï¼‰
                context = self.extract_context_around_email(html_content, email) if html_content else {}

                valid_emails.append({
                    'email': email,
                    'is_personal': self.is_personal_email(email),
                    'name': context.get('name'),
                    'title': context.get('title'),
                    'department': context.get('department')
                })

                domain = email.split('@')[1]
                self.search_stats['unique_domains'].add(domain)

                email_type = "ä¸ªäºº" if self.is_personal_email(email) else "é€šç”¨"
                self.logger.info(f"   âœ… å‘ç°{email_type}é‚®ç®±: {email} (æ¥æº: {source[:30]})")
                if context.get('name'):
                    self.logger.info(f"      ğŸ‘¤ å§“å: {context['name']}")
                if context.get('title'):
                    self.logger.info(f"      ğŸ’¼ èŒä½: {context['title']}")
                if context.get('department'):
                    self.logger.info(f"      ğŸ¢ éƒ¨é—¨: {context['department']}")

        if excluded_count > 0:
            self.logger.debug(f"   ğŸ—‘ï¸ æ’é™¤äº†{excluded_count}ä¸ªç¤ºä¾‹/æ— æ•ˆé‚®ç®±")

        # ä¼˜å…ˆè¿”å›ä¸ªäººé‚®ç®±
        personal_emails = [e for e in valid_emails if e['is_personal']]
        generic_emails = [e for e in valid_emails if not e['is_personal']]

        # ä¸ªäººé‚®ç®± + é€šç”¨é‚®ç®±ï¼ˆæœ‰ä¸Šä¸‹æ–‡çš„ä¼˜å…ˆï¼‰
        generic_with_context = [e for e in generic_emails if e.get('name') or e.get('title') or e.get('department')]
        generic_without_context = [e for e in generic_emails if not (e.get('name') or e.get('title') or e.get('department'))]

        prioritized_emails = personal_emails + generic_with_context + generic_without_context

        self.logger.info(f"   ğŸ“Š é‚®ç®±åˆ†ç±»: {len(personal_emails)}ä¸ªäºº + {len(generic_with_context)}é€šç”¨(æœ‰ä¸Šä¸‹æ–‡) + {len(generic_without_context)}é€šç”¨(æ— ä¸Šä¸‹æ–‡)")

        return prioritized_emails
    
    def validate_email_format(self, email):
        """éªŒè¯é‚®ç®±æ ¼å¼"""
        if not (5 < len(email) < 100 and email.count('@') == 1):
            return False

        local, domain = email.split('@')

        # æ£€æŸ¥æœ¬åœ°éƒ¨åˆ†
        if not local or len(local) > 64:
            return False

        # æ£€æŸ¥åŸŸåéƒ¨åˆ†
        if not domain or '.' not in domain or len(domain) < 4:
            return False

        # æ£€æŸ¥é¡¶çº§åŸŸå
        tld = domain.split('.')[-1]
        if len(tld) < 2 or not tld.isalpha():
            return False

        return True

    def validate_email_deliverable(self, email):
        """
        éªŒè¯é‚®ç®±æ˜¯å¦å¯æŠ•é€’ (MXè®°å½•æ£€æŸ¥)
        - æ£€æŸ¥åŸŸåæ˜¯å¦æœ‰MXè®°å½•
        - æ’é™¤å·²çŸ¥æ— æ•ˆ/åƒåœ¾åŸŸå
        - ä¸å‘é€å®é™…é‚®ä»¶ï¼Œä»…éªŒè¯åŸŸå
        """
        try:
            if not email or '@' not in email:
                return False, "Invalid email format"

            domain = email.split('@')[1].lower()

            # å·²çŸ¥æ— æ•ˆ/åƒåœ¾åŸŸååˆ—è¡¨
            invalid_domains = [
                'example.com', 'test.com', 'domain.com', 'yoursite.com', 'company.com',
                'email.com', 'mail.com', 'sample.com', 'demo.com', 'fake.com',
                'placeholder.com', 'invalid.com', 'null.com', 'void.com',
                'tempmail.com', 'throwaway.com', 'disposable.com',
                'mailinator.com', 'guerrillamail.com', '10minutemail.com',
                'yopmail.com', 'tempmail.net', 'trashmail.com'
            ]

            if domain in invalid_domains:
                return False, f"Known invalid domain: {domain}"

            # æ£€æŸ¥MXè®°å½• (åŸŸåæ˜¯å¦å¯æ¥æ”¶é‚®ä»¶)
            try:
                mx_records = dns.resolver.resolve(domain, 'MX')
                if not mx_records:
                    return False, f"No MX records for domain: {domain}"

                # è·å–MXè®°å½•çš„ä¸»æœºå
                mx_hosts = [str(r.exchange).rstrip('.').lower() for r in mx_records]

                # æ£€æŸ¥MXè®°å½•æ˜¯å¦æŒ‡å‘å·²çŸ¥åƒåœ¾é‚®ä»¶æœåŠ¡
                spam_mx_patterns = ['localhost', 'null', 'void', 'invalid', 'example']
                for mx_host in mx_hosts:
                    if any(pattern in mx_host for pattern in spam_mx_patterns):
                        return False, f"MX record points to invalid host: {mx_host}"

                return True, f"Valid MX records found: {len(mx_records)}"

            except dns.resolver.NXDOMAIN:
                return False, f"Domain does not exist: {domain}"
            except dns.resolver.NoAnswer:
                return False, f"No MX records for domain: {domain}"
            except dns.resolver.NoNameservers:
                return False, f"No name servers for domain: {domain}"
            except dns.exception.Timeout:
                # DNSè¶…æ—¶ - å¯èƒ½æ˜¯æœ‰æ•ˆåŸŸåï¼Œä½†æš‚æ—¶æ— æ³•éªŒè¯
                self.logger.warning(f"   â±ï¸ DNS timeout for {domain}, assuming valid")
                return True, "DNS timeout - assuming valid"
            except Exception as e:
                # DNSæŸ¥è¯¢å¤±è´¥ä½†ä¸ä¸€å®šæ— æ•ˆ
                self.logger.warning(f"   âš ï¸ DNS query failed for {domain}: {e}")
                return True, f"DNS query failed, assuming valid: {e}"

        except Exception as e:
            self.logger.error(f"   âŒ Email validation error: {e}")
            return False, f"Validation error: {e}"

    def validate_emails_batch(self, email_list):
        """
        æ‰¹é‡éªŒè¯é‚®ç®±åˆ—è¡¨ï¼Œè¿”å›æœ‰æ•ˆé‚®ç®±
        """
        if not email_list:
            return []

        valid_emails = []
        invalid_count = 0

        self.logger.info(f"ğŸ” éªŒè¯ {len(email_list)} ä¸ªé‚®ç®±åœ°å€...")

        for email_data in email_list:
            email = email_data['email'] if isinstance(email_data, dict) else email_data

            is_valid, reason = self.validate_email_deliverable(email)

            if is_valid:
                valid_emails.append(email_data)
            else:
                invalid_count += 1
                self.logger.info(f"   âŒ æ’é™¤æ— æ•ˆé‚®ç®±: {email} ({reason})")

        self.logger.info(f"âœ… éªŒè¯å®Œæˆ: {len(valid_emails)} æœ‰æ•ˆ, {invalid_count} æ— æ•ˆ")
        return valid_emails
    
    def scrape_website_advanced(self, url):
        """é«˜çº§ç½‘ç«™çˆ¬å– - ä¸“æ³¨è”ç³»ä¿¡æ¯ï¼Œæ— æ—¶é—´é™åˆ¶ + ä¸Šä¸‹æ–‡æå–"""
        try:
            self.logger.info(f"   ğŸŒ æ·±åº¦æ— é™çˆ¬å–: {url[:60]}...")
            self.search_stats['websites_scraped'] += 1

            start_time = time.time()
            # ç§»é™¤è¶…æ—¶é™åˆ¶ - è®©çˆ¬å–æœ‰å……è¶³æ—¶é—´
            response = self.session.get(url)
            duration = time.time() - start_time

            if response.status_code != 200:
                self.logger.warning(f"   âš ï¸ HTTP {response.status_code}: {url}")
                return []

            soup = BeautifulSoup(response.content, 'html.parser')

            # ä¿å­˜åŸå§‹HTMLç”¨äºä¸Šä¸‹æ–‡æå–
            html_content = response.content

            # ç§»é™¤å¹²æ‰°å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                element.decompose()

            # ä¼˜å…ˆæœç´¢è”ç³»ç›¸å…³åŒºåŸŸ
            priority_areas = []

            # æŸ¥æ‰¾è”ç³»é¡µé¢å…³é”®åŒºåŸŸ
            contact_selectors = [
                '[class*="contact"]', '[id*="contact"]',
                '[class*="about"]', '[id*="about"]',
                '[class*="team"]', '[id*="team"]',
                '[class*="staff"]', '[id*="staff"]',
                '[class*="press"]', '[id*="press"]',
                '[class*="media"]', '[id*="media"]'
            ]

            for selector in contact_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    priority_areas.append(elem.get_text())

            # è·å–ä¸»è¦å†…å®¹
            main_content = soup.get_text()

            # åˆå¹¶æ‰€æœ‰æ–‡æœ¬ï¼Œä¼˜å…ˆå¤„ç†è”ç³»åŒºåŸŸ
            all_text = ' '.join(priority_areas) + ' ' + main_content

            # ä¼ é€’HTMLå†…å®¹ä»¥æå–ä¸Šä¸‹æ–‡
            emails = self.extract_emails_advanced(all_text, f"ç½‘ç«™ {url}", html_content)

            self.logger.info(f"   âœ… çˆ¬å–å®Œæˆ ({duration:.1f}s): {len(emails)}ä¸ªé‚®ç®±")
            return emails

        except Exception as e:
            self.logger.error(f"   âŒ çˆ¬å–å¤±è´¥ {url}: {str(e)}")
            return []
    
    def execute_persistent_discovery(self, industry, target_count=5, max_rounds=None, session_id=None):
        """æ‰§è¡Œæ— é™åˆ¶æŒç»­æœç´¢ - è¶Šå¤šè¶Šå‡†ç¡®"""
        # ğŸ”¥ FIX: Scale max_rounds based on target_count
        # Each round finds ~5-15 new emails on average (after filtering cached)
        # Use at least 100 rounds, scale up for larger requests, cap at 500 for safety
        if max_rounds is None:
            max_rounds = min(500, max(100, target_count // 5))  # ~5 emails per round, max 500 rounds

        self.logger.info(f"ğŸš€ å¯åŠ¨æ— é™åˆ¶è¶…çº§é‚®ç®±æœç´¢ - {industry}")
        self.logger.info(f"   ğŸ¯ ç›®æ ‡: {target_count}ä¸ªNEWé‚®ç®± (è·³è¿‡å·²è¿”å›)")
        self.logger.info(f"   ğŸ”„ æœ€å¤§è½®æ•°: {max_rounds} (åŠ¨æ€è°ƒæ•´ï¼Œç¡®ä¿æ‰¾åˆ°è¶³å¤Ÿæ–°é‚®ç®±)")
        self.logger.info(f"   ğŸ“Š ä½¿ç”¨2024å¹´æœ€ä½³æœç´¢å®è·µ")
        self.logger.info(f"   â° æ— æ—¶é—´é™åˆ¶ - æŒç»­æœç´¢ç›´åˆ°æ‰¾åˆ°è¶³å¤Ÿæ–°é‚®ç®±")
        if session_id:
            self.logger.info(f"   ğŸ”‘ Session ID: {session_id} (campaign-specific cache)")

        # ğŸ”¥ FIX: Load cache of already-returned emails with session_id
        cached_count = self.load_returned_emails_cache(industry, session_id)
        if cached_count > 0:
            self.logger.info(f"   ğŸ”„ è·³è¿‡å·²è¿”å›çš„ {cached_count} ä¸ªé‚®ç®±ï¼Œå¯»æ‰¾æ–°é‚®ç®±...")

        start_time = time.time()
        all_emails = []
        round_num = 1
        consecutive_empty_rounds = 0
        total_emails_found = 0  # ğŸ”¥ FIX: Track total including duplicates
        total_cached_skipped = 0  # ğŸ”¥ FIX: Track how many cached emails skipped
        
        while len(all_emails) < target_count and round_num <= max_rounds:
            self.logger.info(f"\nğŸ“ ç¬¬{round_num}è½®æœç´¢ (å·²æ‰¾åˆ° {len(all_emails)}/{target_count})")
            
            # ç”Ÿæˆæœ¬è½®ç­–ç•¥
            strategies = self.generate_professional_search_strategies(industry, round_num)
            round_emails = []
            
            for i, strategy in enumerate(strategies, 1):
                self.logger.info(f"   ğŸ¯ ç­–ç•¥{i}/{len(strategies)}: {strategy[:70]}...")
                
                
                # æœç´¢
                results = self.search_with_advanced_logging(strategy)
                
                if not results:
                    self.logger.warning(f"   âš ï¸ ç­–ç•¥{i} æ— ç»“æœ")
                    continue
                
                # ä»æœç´¢é¢„è§ˆæå–é‚®ç®±
                preview_emails = []
                for result in results:
                    text = f"{result.get('title', '')} {result.get('content', '')}"
                    emails = self.extract_emails_advanced(text, f"æœç´¢é¢„è§ˆ {i}")

                    for email_data in emails:
                        total_emails_found += 1  # ğŸ”¥ FIX: Count all emails found
                        email_addr = email_data['email']
                        # ğŸ”¥ NEW: Skip already-returned emails
                        if email_addr in self.already_returned_emails:
                            total_cached_skipped += 1  # ğŸ”¥ FIX: Track skipped
                            continue
                        if not any(e['email'] == email_addr for e in preview_emails):
                            preview_emails.append({
                                'email': email_addr,
                                'name': email_data.get('name'),
                                'title': email_data.get('title'),
                                'department': email_data.get('department'),
                                'is_personal': email_data.get('is_personal', False),
                                'source': 'search_preview',
                                'source_url': result.get('url', ''),
                                'source_title': result.get('title', ''),
                                'confidence': 0.9 if email_data.get('is_personal') else 0.7,
                                'round': round_num,
                                'strategy': strategy,
                                'discovery_method': 'professional_search'
                            })
                
                round_emails.extend(preview_emails)
                self.logger.info(f"   ğŸ“§ ç­–ç•¥{i}é¢„è§ˆ: {len(preview_emails)}ä¸ªé‚®ç®±")
                
                # å¹¶è¡Œçˆ¬å–æ›´å¤šç½‘ç«™ - æ— é™åˆ¶æ¨¡å¼
                promising_sites = [r for r in results[:20] 
                                 if any(word in r.get('url', '').lower() 
                                       for word in ['contact', 'about', 'team', 'press'])]
                
                if not promising_sites:
                    promising_sites = results[:15]  # å¢åŠ å¤‡é€‰æ–¹æ¡ˆæ•°é‡
                
                self.logger.info(f"   ğŸŒ æ·±åº¦å¹¶è¡Œçˆ¬å–{len(promising_sites)}ä¸ªç½‘ç«™ (æ— æ—¶é—´é™åˆ¶)...")
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:
                    future_to_result = {
                        executor.submit(self.scrape_website_advanced, site['url']): site 
                        for site in promising_sites
                    }
                    
                    # ç§»é™¤è¶…æ—¶é™åˆ¶ï¼Œè®©æ‰€æœ‰ç½‘ç«™éƒ½æœ‰å……è¶³æ—¶é—´å®Œæˆ
                    for future in concurrent.futures.as_completed(future_to_result):
                        try:
                            site = future_to_result[future]
                            website_emails = future.result()

                            for email in website_emails:
                                total_emails_found += 1  # ğŸ”¥ FIX: Count all emails found
                                # ğŸ”¥ NEW: Skip already-returned emails
                                if email in self.already_returned_emails:
                                    total_cached_skipped += 1  # ğŸ”¥ FIX: Track skipped
                                    continue
                                if not any(e['email'] == email for e in round_emails):
                                    round_emails.append({
                                        'email': email,
                                        'source': 'website_scraping',
                                        'source_url': site['url'],
                                        'source_title': site.get('title', ''),
                                        'confidence': 0.95,
                                        'round': round_num,
                                        'strategy': strategy,
                                        'discovery_method': 'deep_scraping'
                                    })
                        except Exception as e:
                            continue
                
                # æ£€æŸ¥è¿›åº¦ï¼Œä½†ä¸ç«‹å³åœæ­¢ - è®©å®ƒç»§ç»­æœç´¢æ›´å¤š
                all_unique = {e['email']: e for e in all_emails + round_emails}
                if len(all_unique) >= target_count:
                    self.logger.info(f"ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡ï¼Œä½†ç»§ç»­æœç´¢ä»¥è·å¾—æ›´å‡†ç¡®ç»“æœ...")
                    # ä¸breakï¼Œç»§ç»­æœç´¢
                
                time.sleep(0.3)  # å‡å°‘ç­–ç•¥é—´éš”
            
            # æ›´æ–°æ€»é‚®ç®±åˆ—è¡¨
            all_emails.extend(round_emails)
            all_unique = {e['email']: e for e in all_emails}
            all_emails = list(all_unique.values())

            # ğŸ”¥ FIX: Show detailed statistics including cached skips
            self.logger.info(f"ğŸ“Š ç¬¬{round_num}è½®ç»“æœ: æ–°å¢{len(round_emails)}ä¸ªï¼Œæ€»è®¡{len(all_emails)}ä¸ªNEWé‚®ç®±")
            if total_cached_skipped > 0:
                self.logger.info(f"   ğŸ”„ å·²è·³è¿‡ {total_cached_skipped} ä¸ªé‡å¤/ç¼“å­˜é‚®ç®± (æ€»å‘ç°{total_emails_found}ä¸ª)")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´ç­–ç•¥ï¼Œä½†ä¸è½»æ˜“æ”¾å¼ƒ
            if len(round_emails) == 0:
                consecutive_empty_rounds += 1
                self.logger.warning(f"âš ï¸ è¿ç»­{consecutive_empty_rounds}è½®æ— ç»“æœ - ç»§ç»­å°è¯•")
                
                if consecutive_empty_rounds >= 5:  # å¢åŠ å®¹å¿åº¦
                    self.logger.info("ğŸ”„ åˆ‡æ¢åˆ°æ›´å¹¿æ³›çš„æœç´¢ç­–ç•¥...")
            else:
                consecutive_empty_rounds = 0
            
            # å³ä½¿è¾¾åˆ°ç›®æ ‡ä¹Ÿä¸ç«‹å³é€€å‡º - ç»§ç»­æœç´¢è·å¾—æ›´å¤šé‚®ç®±
            if len(all_emails) >= target_count and round_num >= 5:
                self.logger.info(f"ğŸ¯ å·²æ”¶é›†è¶³å¤Ÿé‚®ç®±å¹¶è¿›è¡Œäº†å……åˆ†æœç´¢ï¼Œå‡†å¤‡ç»“æŸ")
                break
            
            round_num += 1
            if round_num <= max_rounds:
                time.sleep(1)  # å‡å°‘è½®æ¬¡é—´éš”
        
        # æ•´ç†æœ€ç»ˆç»“æœ
        preliminary_emails = all_emails[:target_count + 10]  # Get extras in case some fail validation
        total_time = time.time() - start_time

        # ğŸ”¥ NEW: Validate emails before returning (only for batch search, not quick search)
        self.logger.info(f"\nğŸ” éªŒè¯é‚®ç®±æœ‰æ•ˆæ€§ (MXè®°å½•æ£€æŸ¥)...")
        validated_emails = self.validate_emails_batch(preliminary_emails)
        invalid_count = len(preliminary_emails) - len(validated_emails)
        if invalid_count > 0:
            self.logger.info(f"   ğŸ—‘ï¸ æ’é™¤äº† {invalid_count} ä¸ªæ— æ•ˆé‚®ç®± (æ— MXè®°å½•æˆ–æ— æ•ˆåŸŸå)")

        # Take only the target count after validation
        final_emails = validated_emails[:target_count]

        # æ›´æ–°ç»Ÿè®¡
        self.search_stats['emails_found'] = len(final_emails)
        self.search_stats['invalid_emails_filtered'] = invalid_count

        # ğŸ”¥ FIX: Save newly returned emails to cache with session_id
        new_email_addresses = [e['email'] for e in final_emails]
        if new_email_addresses:
            self.save_returned_emails_cache(industry, new_email_addresses, session_id)
            self.logger.info(f"   âœ… å·²ä¿å­˜ {len(new_email_addresses)} ä¸ªéªŒè¯åé‚®ç®±åˆ°ç¼“å­˜")

        self.logger.info(f"\nğŸŠ è¶…çº§æœç´¢å®Œæˆï¼")
        self.logger.info(f"   ğŸ“§ æœ€ç»ˆé‚®ç®±: {len(final_emails)}ä¸ªNEWé‚®ç®± (å…¨éƒ¨éªŒè¯æœ‰æ•ˆ)")
        self.logger.info(f"   ğŸ”„ æœç´¢è½®æ•°: {round_num-1}")
        self.logger.info(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’")
        self.logger.info(f"   ğŸ“Š æˆåŠŸç‡: {self.search_stats['successful_queries']}/{self.search_stats['total_queries']}")
        self.logger.info(f"   ğŸŒ çˆ¬å–ç½‘ç«™: {self.search_stats['websites_scraped']}ä¸ª")
        self.logger.info(f"   ğŸ¢ å‘ç°åŸŸå: {len(self.search_stats['unique_domains'])}ä¸ª")
        self.logger.info(f"   ğŸ”„ æ€»å‘ç°: {total_emails_found}ä¸ª (è·³è¿‡{total_cached_skipped}ä¸ªé‡å¤)")
        self.logger.info(f"   ğŸ—‘ï¸ æ— æ•ˆè¿‡æ»¤: {invalid_count}ä¸ª (æ— MXè®°å½•æˆ–æ— æ•ˆåŸŸå)")
        self.logger.info(f"   ğŸ—‚ï¸ ç¼“å­˜æ€»æ•°: {len(self.already_returned_emails)} ä¸ªå†å²é‚®ç®±")

        # æ˜¾ç¤ºå‘ç°çš„é‚®ç®±
        if final_emails:
            self.logger.info("\nğŸ“§ å‘ç°çš„é‚®ç®±åœ°å€ (æ–°):")
            for i, email_data in enumerate(final_emails, 1):
                self.logger.info(f"   {i}. {email_data['email']} (ç½®ä¿¡åº¦: {email_data['confidence']})")

        return {
            'success': len(final_emails) > 0,
            'emails': [e['email'] for e in final_emails],
            'email_details': final_emails,
            'total_emails': len(final_emails),
            'search_rounds': round_num - 1,
            'execution_time': total_time,
            'search_stats': self.prepare_stats_for_json(),
            'industry': industry,
            'target_achieved': len(final_emails) >= target_count,
            'method': 'super_email_discovery_2024',
            'confidence_score': sum(e['confidence'] for e in final_emails) / len(final_emails) if final_emails else 0,
            'timestamp': datetime.now().isoformat()
        }
    
    def prepare_stats_for_json(self):
        """å‡†å¤‡ç»Ÿè®¡æ•°æ®ç”¨äºJSONåºåˆ—åŒ–"""
        stats = dict(self.search_stats)
        stats['unique_domains'] = list(self.search_stats['unique_domains'])
        return stats

def main():
    if len(sys.argv) < 2:
        print('ä½¿ç”¨æ–¹æ³•: python3 SuperEmailDiscoveryEngine.py "è¡Œä¸šåç§°" [é‚®ç®±æ•°é‡] [session_id]')
        print('ç¤ºä¾‹: python3 SuperEmailDiscoveryEngine.py "AI startup" 5 campaign_123')
        return

    industry = sys.argv[1]
    target_count = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    session_id = sys.argv[3] if len(sys.argv) > 3 else None  # ğŸ”¥ FIX: Accept session_id

    engine = SuperEmailDiscoveryEngine()
    # ğŸ”¥ FIX: Let max_rounds be calculated dynamically based on target_count
    results = engine.execute_persistent_discovery(industry, target_count, session_id=session_id)
    
    print("\n" + "="*90)
    print("ğŸ¯ è¶…çº§é‚®ç®±æœç´¢å¼•æ“ - æœ€ç»ˆæŠ¥å‘Š")
    print("="*90)
    
    if results['success']:
        print(f"âœ… æˆåŠŸå‘ç° {results['total_emails']} ä¸ªé‚®ç®±åœ°å€:")
        for i, email in enumerate(results['emails'], 1):
            print(f"   {i}. {email}")
        
        print(f"\nğŸ“Š æœç´¢æ€§èƒ½æŒ‡æ ‡:")
        print(f"   ğŸ”„ æœç´¢è½®æ•°: {results['search_rounds']}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {results['execution_time']:.1f}ç§’")
        print(f"   ğŸ¯ ç›®æ ‡è¾¾æˆ: {'æ˜¯' if results['target_achieved'] else 'å¦'}")
        print(f"   ğŸ“ˆ æŸ¥è¯¢æˆåŠŸç‡: {results['search_stats']['successful_queries']}/{results['search_stats']['total_queries']}")
        print(f"   ğŸŒ ç½‘ç«™çˆ¬å–: {results['search_stats']['websites_scraped']}ä¸ª")
        print(f"   ğŸ¢ å‘ç°åŸŸå: {len(results['search_stats']['unique_domains'])}ä¸ª")
        print(f"   ğŸ­ å¹³å‡ç½®ä¿¡åº¦: {results['confidence_score']:.2f}")
        
    else:
        print("âŒ æœªèƒ½å‘ç°é‚®ç®±åœ°å€")
        print("ğŸ’¡ å»ºè®®ï¼šå°è¯•æ›´å…·ä½“çš„è¡Œä¸šæè¿°æˆ–å¢åŠ æœç´¢è½®æ•°")
    
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ (JSON):")
    print(json.dumps(results, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()