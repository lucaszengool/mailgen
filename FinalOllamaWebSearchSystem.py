#!/usr/bin/env python3
"""
Final Ollama Web Search System
æœ€ç»ˆç‰ˆæœ¬ï¼šè®©Ollamaç›´æ¥å…·å¤‡è”ç½‘æœç´¢èƒ½åŠ›å¹¶è¿”å›çœŸå®é‚®ç®±
- æœ¬åœ°Ollama LLM (qwen2.5:0.5bå¿«é€Ÿæ¨¡å‹)
- ç›´æ¥ç½‘ç»œæœç´¢èƒ½åŠ› (DuckDuckGo + ç½‘ç«™çˆ¬å–)
- æ™ºèƒ½é‚®ç®±å‘ç°å’ŒéªŒè¯
- ç«‹å³è¿”å›æ‰¾åˆ°çš„é‚®ç®±
- å®Œå…¨è‡ªä¸»è¿è¡Œ
"""

import sys
import json
import time
import re
import requests
import os
from datetime import datetime
from urllib.parse import quote, unquote
from bs4 import BeautifulSoup
import random
import concurrent.futures

class FinalOllamaWebSearchSystem:
    def __init__(self):
        # æœ¬åœ°Ollamaé…ç½®
        self.ollama_url = 'http://localhost:11434'
        
        # ç½‘ç»œæœç´¢é…ç½®
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
        
        # é‚®ç®±åŒ¹é…å’ŒéªŒè¯
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        
        print("ğŸ¤– Final Ollama Web Search System åˆå§‹åŒ–")
        print("   ğŸ§  AIå¼•æ“: æœ¬åœ°Ollama (qwen2.5:0.5b)")
        print("   ğŸŒ æœç´¢èƒ½åŠ›: ç›´æ¥ç½‘ç»œæœç´¢")
        print("   ğŸ“§ é‚®ç®±å‘ç°: å®æ—¶æ™ºèƒ½æå–")
        print("   âš¡ ç‰¹ç‚¹: Ollamaå…·å¤‡å®Œæ•´è”ç½‘èƒ½åŠ›")
        
    def generate_search_queries_with_ollama(self, industry):
        """ä½¿ç”¨Ollamaç”Ÿæˆæ™ºèƒ½æœç´¢æŸ¥è¯¢"""
        try:
            print(f"ğŸ§  Ollamaç”Ÿæˆ'{industry}'è¡Œä¸šçš„æœç´¢ç­–ç•¥...")
            
            prompt = f"""ä¸º{industry}è¡Œä¸šç”Ÿæˆ5ä¸ªä¸åŒçš„ç½‘ç»œæœç´¢æŸ¥è¯¢æ¥æ‰¾åˆ°å…¬å¸é‚®ç®±åœ°å€ã€‚

è¦æ±‚:
1. æ¯ä¸ªæŸ¥è¯¢éƒ½åº”è¯¥èƒ½æ‰¾åˆ°çœŸå®çš„å•†ä¸šé‚®ç®±
2. é’ˆå¯¹ä¸åŒç±»å‹çš„è”ç³»äºº(CEO, é”€å”®, å®¢æœç­‰)
3. åŒ…å«å…·ä½“çš„è¡Œä¸šå…³é”®è¯
4. é€‚åˆåœ¨æœç´¢å¼•æ“ä¸­ä½¿ç”¨

è¡Œä¸š: {industry}

è¯·è¿”å›5ä¸ªæœç´¢æŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ª:
1. 
2. 
3. 
4. 
5. 

åªè¿”å›æŸ¥è¯¢ï¼Œä¸è¦è§£é‡Š:"""

            response = requests.post(f"{self.ollama_url}/api/generate", json={
                'model': 'qwen2.5:0.5b',
                'prompt': prompt,
                'stream': False,
                'options': {'temperature': 0.7, 'num_ctx': 1024}
            }, timeout=30)
            
            if response.status_code == 200:
                result = response.json()['response'].strip()
                
                # æå–æŸ¥è¯¢
                queries = []
                for line in result.split('\n'):
                    line = line.strip()
                    if line and (line.startswith(('1.', '2.', '3.', '4.', '5.')) or len(line) > 10):
                        # æ¸…ç†è¡Œå·
                        cleaned = re.sub(r'^\d+\.\s*', '', line).strip()
                        if cleaned and len(cleaned) > 5:
                            queries.append(cleaned)
                
                if queries:
                    print(f"   âœ… Ollamaç”Ÿæˆäº†{len(queries)}ä¸ªæœç´¢æŸ¥è¯¢")
                    return queries[:5]
            
            # å¤‡ç”¨æŸ¥è¯¢
            print(f"   âš ï¸  ä½¿ç”¨å¤‡ç”¨æœç´¢æŸ¥è¯¢")
            return [
                f"{industry} company contact email address",
                f"{industry} business CEO founder email",
                f"{industry} company customer service email",
                f"{industry} startup contact information",
                f"{industry} company sales email directory"
            ]
            
        except Exception as e:
            print(f"   âŒ OllamaæŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {str(e)}")
            return [f"{industry} company email contact"]
    
    def search_web_for_emails(self, query):
        """æ‰§è¡Œç½‘ç»œæœç´¢å¹¶ç›´æ¥æå–é‚®ç®±"""
        try:
            print(f"   ğŸ” ç½‘ç»œæœç´¢: {query}")
            
            # DuckDuckGoæœç´¢
            search_url = f"https://duckduckgo.com/html/?q={quote(query)}"
            response = self.session.get(search_url, timeout=15)
            
            all_emails = []
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ä»æœç´¢ç»“æœä¸­ç›´æ¥æå–é‚®ç®±
                page_text = soup.get_text()
                preview_emails = self.extract_valid_emails(page_text)
                
                if preview_emails:
                    print(f"      ğŸ“§ æœç´¢é¢„è§ˆä¸­æ‰¾åˆ°{len(preview_emails)}ä¸ªé‚®ç®±")
                    for email in preview_emails:
                        all_emails.append({
                            'email': email,
                            'source': 'search_preview',
                            'query': query
                        })
                
                # æå–æœç´¢ç»“æœé“¾æ¥
                result_links = []
                for link in soup.find_all('a', class_='result__a')[:8]:
                    try:
                        url = link.get('href', '')
                        title = link.get_text().strip()
                        if url and 'duckduckgo.com' not in url:
                            result_links.append({'url': url, 'title': title})
                    except:
                        continue
                
                # å¹¶è¡Œçˆ¬å–ç½‘ç«™
                if result_links:
                    print(f"      ğŸŒ çˆ¬å–{len(result_links)}ä¸ªç½‘ç«™...")
                    
                    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                        future_to_url = {
                            executor.submit(self.scrape_website_for_emails, link['url']): link 
                            for link in result_links
                        }
                        
                        for future in concurrent.futures.as_completed(future_to_url):
                            try:
                                link = future_to_url[future]
                                emails = future.result()
                                
                                if emails:
                                    print(f"         âœ… {link['title'][:30]}... æ‰¾åˆ°{len(emails)}ä¸ªé‚®ç®±")
                                    for email in emails:
                                        all_emails.append({
                                            'email': email,
                                            'source': 'website_scraping',
                                            'source_url': link['url'],
                                            'source_title': link['title'],
                                            'query': query
                                        })
                                        
                            except Exception as e:
                                continue
            
            # å»é‡
            unique_emails = {}
            for email_data in all_emails:
                email = email_data['email']
                if email not in unique_emails:
                    unique_emails[email] = email_data
            
            return list(unique_emails.values())
            
        except Exception as e:
            print(f"      âŒ æœç´¢é”™è¯¯: {str(e)}")
            return []
    
    def scrape_website_for_emails(self, url):
        """çˆ¬å–å•ä¸ªç½‘ç«™å¯»æ‰¾é‚®ç®±"""
        try:
            response = self.session.get(url, timeout=8)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ç§»é™¤æ— ç”¨å…ƒç´ 
                for element in soup(["script", "style", "nav", "footer"]):
                    element.decompose()
                
                text = soup.get_text()
                emails = self.extract_valid_emails(text)
                
                return emails
            else:
                return []
                
        except Exception as e:
            return []
    
    def extract_valid_emails(self, text):
        """ä»æ–‡æœ¬ä¸­æå–æœ‰æ•ˆçš„å•†ä¸šé‚®ç®±"""
        emails = self.email_pattern.findall(text)
        
        valid_emails = []
        for email in emails:
            email_lower = email.lower()
            
            # æ’é™¤å‡é‚®ç®±
            if any(pattern in email_lower for pattern in [
                'example.com', 'test.com', 'domain.com', 'yoursite.com',
                'noreply', 'no-reply', 'donotreply', 'privacy@', 'legal@',
                'support@example', 'admin@example', 'info@example'
            ]):
                continue
            
            # åŸºæœ¬éªŒè¯
            if 5 < len(email) < 100 and email.count('@') == 1:
                domain = email.split('@')[1]
                if '.' in domain and len(domain) > 4:
                    valid_emails.append(email)
        
        return valid_emails
    
    def analyze_found_emails_with_ollama(self, emails_data):
        """ä½¿ç”¨Ollamaåˆ†ææ‰¾åˆ°çš„é‚®ç®±å¹¶ç”Ÿæˆæ€»ç»“"""
        try:
            if not emails_data:
                return "æœªæ‰¾åˆ°ä»»ä½•é‚®ç®±"
            
            print(f"   ğŸ§  Ollamaåˆ†æ{len(emails_data)}ä¸ªé‚®ç®±...")
            
            # å‡†å¤‡é‚®ç®±ä¿¡æ¯
            email_info = []
            for data in emails_data:
                email_info.append(f"- {data['email']} (æ¥æº: {data['source']})")
            
            prompt = f"""åˆ†æä»¥ä¸‹å‘ç°çš„é‚®ç®±åœ°å€ï¼Œæä¾›ä¸“ä¸šæ€»ç»“:

å‘ç°çš„é‚®ç®±:
{chr(10).join(email_info)}

è¯·æä¾›:
1. é‚®ç®±æ€»æ•°
2. ä¸»è¦æ¥æºç±»å‹
3. æ¨æµ‹çš„è”ç³»äººç±»å‹(CEO, é”€å”®, å®¢æœç­‰)
4. å»ºè®®çš„è”ç³»ä¼˜å…ˆçº§

ç®€æ´å›ç­”ï¼Œé‡ç‚¹çªå‡ºå®ç”¨ä¿¡æ¯:"""

            response = requests.post(f"{self.ollama_url}/api/generate", json={
                'model': 'qwen2.5:0.5b',
                'prompt': prompt,
                'stream': False,
                'options': {'temperature': 0.3, 'num_ctx': 1024}
            }, timeout=20)
            
            if response.status_code == 200:
                analysis = response.json()['response'].strip()
                print(f"   âœ… Ollamaåˆ†æå®Œæˆ")
                return analysis
            else:
                return "Ollamaåˆ†æå¤±è´¥"
                
        except Exception as e:
            print(f"   âŒ Ollamaåˆ†æé”™è¯¯: {str(e)}")
            return f"æ‰¾åˆ°{len(emails_data)}ä¸ªé‚®ç®±ï¼Œåˆ†æåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨"
    
    def find_emails_with_ollama_web_search(self, industry, max_emails=5):
        """ä½¿ç”¨Ollamaç½‘ç»œæœç´¢åŠŸèƒ½å‘ç°é‚®ç®±"""
        print(f"ğŸ¤– å¯åŠ¨Ollamaç½‘ç»œæœç´¢é‚®ç®±å‘ç°: {industry}")
        print(f"ğŸ¯ ç›®æ ‡: {max_emails}ä¸ªé‚®ç®±")
        print("=" * 60)
        
        # 1. ä½¿ç”¨Ollamaç”Ÿæˆæœç´¢ç­–ç•¥
        search_queries = self.generate_search_queries_with_ollama(industry)
        
        all_emails = []
        
        # 2. æ‰§è¡Œæ¯ä¸ªæœç´¢æŸ¥è¯¢
        for i, query in enumerate(search_queries, 1):
            print(f"\nğŸ“ æœç´¢ç­–ç•¥ {i}/{len(search_queries)}")
            
            emails = self.search_web_for_emails(query)
            
            if emails:
                all_emails.extend(emails)
                print(f"   âœ… æœ¬æ¬¡æœç´¢æ‰¾åˆ°{len(emails)}ä¸ªé‚®ç®±")
                
                # ç«‹å³è¿”å›æ‰¾åˆ°çš„é‚®ç®±ï¼ˆæ ¹æ®ç”¨æˆ·è¦æ±‚ï¼‰
                unique_emails = {}
                for email_data in all_emails:
                    email = email_data['email']
                    if email not in unique_emails:
                        unique_emails[email] = email_data
                
                if len(unique_emails) >= max_emails:
                    print(f"   ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡é‚®ç®±æ•°é‡ï¼Œç«‹å³è¿”å›ç»“æœ")
                    break
            else:
                print(f"   âš ï¸  æœ¬æ¬¡æœç´¢æœªæ‰¾åˆ°é‚®ç®±")
            
            # æœç´¢é—´éš”
            time.sleep(2)
        
        # 3. å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_emails = {}
        for email_data in all_emails:
            email = email_data['email']
            if email not in unique_emails:
                unique_emails[email] = email_data
        
        final_emails = list(unique_emails.values())[:max_emails]
        
        # 4. ä½¿ç”¨Ollamaåˆ†æç»“æœ
        analysis = self.analyze_found_emails_with_ollama(final_emails)
        
        print(f"\nğŸ‰ æœç´¢å®Œæˆ!")
        print(f"   ğŸ“§ æ‰¾åˆ°é‚®ç®±: {len(final_emails)}ä¸ª")
        
        return {
            'success': True,
            'emails': [e['email'] for e in final_emails],
            'email_details': final_emails,
            'analysis': analysis,
            'search_queries': search_queries,
            'total_emails': len(final_emails)
        }

def main():
    if len(sys.argv) < 2:
        print(json.dumps({'error': 'è¯·æä¾›è¡Œä¸šåç§° (ä¾‹å¦‚: "AI startup", "fruit companies")'}))
        return
    
    industry = sys.argv[1]
    max_emails = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    search_system = FinalOllamaWebSearchSystem()
    
    # æ‰§è¡Œæœç´¢
    results = search_system.find_emails_with_ollama_web_search(industry, max_emails)
    
    # å‡†å¤‡è¾“å‡º
    output = {
        'success': results['success'],
        'emails': results.get('emails', []),
        'email_details': results.get('email_details', []),
        'total_emails': results.get('total_emails', 0),
        'analysis': results.get('analysis', ''),
        'search_queries': results.get('search_queries', []),
        'industry': industry,
        'search_method': 'final_ollama_web_search',
        'ollama_enabled': True,
        'web_search_enabled': True,
        'real_time_discovery': True,
        'timestamp': datetime.now().isoformat()
    }
    
    print("\n" + "=" * 60)
    print("ğŸ¤– Final Ollama Web Search System ç»“æœ")
    print("=" * 60)
    
    if results['success']:
        print("ğŸ“§ å‘ç°çš„çœŸå®é‚®ç®±:")
        for email in results['emails']:
            print(f"   ğŸ“§ {email}")
        
        print(f"\nğŸ§  Ollamaæ™ºèƒ½åˆ†æ:")
        print(f"   {results['analysis']}")
        
        print(f"\nğŸ“Š æœç´¢ç»Ÿè®¡:")
        print(f"   ğŸ“§ é‚®ç®±æ€»æ•°: {results['total_emails']}")
        print(f"   ğŸ” æœç´¢æŸ¥è¯¢: {len(results['search_queries'])}ä¸ª")
        print(f"   ğŸ§  AIå¼•æ“: Ollama (æœ¬åœ°)")
        print(f"   ğŸŒ æœç´¢å¼•æ“: DuckDuckGo + ç½‘ç«™çˆ¬å–")
        print(f"   âš¡ ç‰¹è‰²: Ollamaç›´æ¥å…·å¤‡è”ç½‘æœç´¢èƒ½åŠ›")
    else:
        print(f"âŒ æœç´¢å¤±è´¥")
    
    print(json.dumps(output, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()