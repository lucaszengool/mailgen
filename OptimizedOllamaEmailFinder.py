#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„Ollama + SearxNGé‚®ç®±æœç´¢ç³»ç»Ÿ
- æŒç»­æœç´¢ç›´åˆ°æ‰¾åˆ°é‚®ç®±
- å¢å¼ºçš„æœç´¢ç­–ç•¥ç”Ÿæˆ
- è¯¦ç»†çš„æ—¥å¿—ç³»ç»Ÿ
- æ™ºèƒ½æŸ¥è¯¢ä¼˜åŒ–
- å¤šè½®æœç´¢æœºåˆ¶
"""

import sys
import json
import time
import re
import requests
import os
import subprocess
from datetime import datetime
from urllib.parse import quote, urlencode
from bs4 import BeautifulSoup
import concurrent.futures
import threading
import logging

class OptimizedOllamaEmailFinder:
    def __init__(self):
        # è®¾ç½®è¯¦ç»†æ—¥å¿—
        self.setup_detailed_logging()
        
        # Ollamaé…ç½®
        self.ollama_url = 'http://localhost:11434'
        self.models = {
            'fast': 'qwen2.5:0.5b',
            'general': 'qwen2.5:0.5b',
            'profile': 'llama3.2'
        }
        
        # SearxNGé…ç½®
        self.searxng_url = 'http://localhost:8080'
        
        # ç½‘ç»œæœç´¢ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
        })
        
        # é‚®ç®±åŒ¹é…æ¨¡å¼
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        
        # æœç´¢çŠ¶æ€è·Ÿè¸ª
        self.found_emails = []
        self.search_attempts = 0
        self.max_search_rounds = 10  # æœ€å¤šæœç´¢è½®æ•°
        self.target_email_count = 5
        self.search_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'emails_found': 0,
            'websites_scraped': 0,
            'unique_domains': set()
        }
        
        self.logger.info("ğŸš€ ä¼˜åŒ–çš„Ollamaé‚®ç®±æœç´¢å™¨åˆå§‹åŒ–")
        self.logger.info(f"   ğŸ§  Fast Model: {self.models['fast']}")
        self.logger.info(f"   ğŸŒ SearxNG: {self.searxng_url}")
        self.logger.info(f"   ğŸ¯ ç›®æ ‡é‚®ç®±æ•°: {self.target_email_count}")
        self.logger.info(f"   ğŸ”„ æœ€å¤šæœç´¢è½®æ•°: {self.max_search_rounds}")
        
    def setup_detailed_logging(self):
        """è®¾ç½®è¯¦ç»†çš„æ—¥å¿—ç³»ç»Ÿ"""
        # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger('EmailFinder')
        self.logger.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler('email_finder.log', encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )
        
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨
        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)
    
    def call_ollama(self, prompt, model_type='fast', options=None):
        """è°ƒç”¨Ollama API - å¢åŠ è¯¦ç»†æ—¥å¿—"""
        try:
            model = self.models.get(model_type, self.models['fast'])
            default_options = {
                'temperature': 0.8,
                'num_predict': 300,
                'num_ctx': 2048
            }
            
            if options:
                default_options.update(options)
                
            self.logger.debug(f"è°ƒç”¨Ollamaæ¨¡å‹: {model}")
            self.logger.debug(f"Prompté•¿åº¦: {len(prompt)}å­—ç¬¦")
            
            start_time = time.time()
            response = requests.post(f"{self.ollama_url}/api/generate", json={
                'model': model,
                'prompt': prompt,
                'stream': False,
                'options': default_options
            }, timeout=60)
            
            duration = time.time() - start_time
            
            if response.status_code == 200:
                result = response.json()['response'].strip()
                self.logger.info(f"âœ… Ollamaå“åº”æˆåŠŸ ({duration:.1f}s) - ç»“æœé•¿åº¦: {len(result)}")
                return result
            else:
                self.logger.error(f"âŒ Ollama APIé”™è¯¯: {response.status_code}")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Ollamaè°ƒç”¨å¤±è´¥: {str(e)}")
            return None
    
    def generate_enhanced_search_strategies(self, industry, round_number=1):
        """ç”Ÿæˆå¢å¼ºçš„æœç´¢ç­–ç•¥ - åŸºäºè½®æ•°è°ƒæ•´ç­–ç•¥"""
        self.logger.info(f"ğŸ§  ç”Ÿæˆç¬¬{round_number}è½®æœç´¢ç­–ç•¥ - è¡Œä¸š: {industry}")
        
        # æ ¹æ®æœç´¢è½®æ•°è°ƒæ•´ç­–ç•¥
        if round_number == 1:
            # ç¬¬ä¸€è½®ï¼šç®€çŸ­é«˜æ•ˆæœç´¢
            strategies = [
                f'{industry} CEO email',
                f'{industry} founder contact',
                f'{industry} business email',
                f'{industry} company contact',
                f'{industry} team email'
            ]
        elif round_number == 2:
            # ç¬¬äºŒè½®ï¼šèŒä½ç›¸å…³æœç´¢
            strategies = [
                f'{industry} sales email',
                f'{industry} marketing contact',
                f'{industry} CTO email',
                f'{industry} director contact',
                f'{industry} manager email'
            ]
        elif round_number == 3:
            # ç¬¬ä¸‰è½®ï¼šä¸“ä¸šè§’è‰²æœç´¢
            strategies = [
                f'{industry} specialist email',
                f'{industry} consultant contact',
                f'{industry} expert email',
                f'{industry} advisor contact',
                f'{industry} support email'
            ]
        else:
            # åç»­è½®æ•°ï¼šæ‰©å±•æœç´¢
            strategies = [
                f'{industry} startup email',
                f'{industry} executive contact',
                f'{industry} owner email',
                f'{industry} info contact',
                f'{industry} partnership email'
            ]
        
        # ä½¿ç”¨Ollamaä¼˜åŒ–æœç´¢ç­–ç•¥
        try:
            prompt = f"""ç”Ÿæˆ5ä¸ªç®€çŸ­çš„{industry}è¡Œä¸šé‚®ç®±æœç´¢æŸ¥è¯¢ï¼š

è¦æ±‚ï¼š
1. æ¯ä¸ªæŸ¥è¯¢æœ€å¤š3-4ä¸ªè¯
2. ä¸ä½¿ç”¨å¤æ‚æ“ä½œç¬¦(site:, intext:ç­‰)
3. ç›´æ¥æœ‰æ•ˆçš„å…³é”®è¯ç»„åˆ
4. åŒ…å«"email"æˆ–"contact"

è¿”å›æ ¼å¼ï¼ˆåªè¿”å›æŸ¥è¯¢ï¼Œä¸è¦è§£é‡Šï¼‰ï¼š
1. [æŸ¥è¯¢1]
2. [æŸ¥è¯¢2]
3. [æŸ¥è¯¢3]
4. [æŸ¥è¯¢4]
5. [æŸ¥è¯¢5]"""

            result = self.call_ollama(prompt, 'fast')
            
            if result:
                optimized_strategies = []
                for line in result.split('\n'):
                    line = line.strip()
                    if line and re.match(r'^\d+\.', line):
                        cleaned = re.sub(r'^\d+\.\s*', '', line).strip()
                        if len(cleaned) > 10:
                            optimized_strategies.append(cleaned)
                
                if len(optimized_strategies) >= 3:
                    self.logger.info(f"âœ… Ollamaä¼˜åŒ–äº†{len(optimized_strategies)}ä¸ªæœç´¢ç­–ç•¥")
                    return optimized_strategies[:5]
        
        except Exception as e:
            self.logger.warning(f"âš ï¸ Ollamaç­–ç•¥ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {str(e)}")
        
        self.logger.info(f"âœ… ä½¿ç”¨é»˜è®¤ç­–ç•¥: {len(strategies)}ä¸ª")
        return strategies
    
    def search_with_enhanced_logging(self, query, max_results=25):
        """å¢å¼ºçš„SearxNGæœç´¢ - è¯¦ç»†æ—¥å¿—"""
        try:
            self.logger.info(f"ğŸ” SearxNGæœç´¢: {query}")
            self.search_stats['total_queries'] += 1
            
            params = {
                'q': query,
                'format': 'json',
                'categories': 'general',
                'pageno': 1
            }
            
            search_url = f"{self.searxng_url}/search"
            start_time = time.time()
            
            response = self.session.get(search_url, params=params, timeout=30)
            duration = time.time() - start_time
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    results = data.get('results', [])
                    
                    self.logger.info(f"âœ… SearxNGæˆåŠŸ ({duration:.1f}s): {len(results)}ä¸ªç»“æœ")
                    self.search_stats['successful_queries'] += 1
                    
                    # åˆ†æç»“æœè´¨é‡
                    email_preview_count = 0
                    for result in results:
                        text = f"{result.get('title', '')} {result.get('content', '')}"
                        if '@' in text:
                            email_preview_count += 1
                    
                    self.logger.info(f"ğŸ“Š ç»“æœåˆ†æ: {email_preview_count}/{len(results)} ä¸ªç»“æœåŒ…å«@ç¬¦å·")
                    
                    formatted_results = []
                    for result in results[:max_results]:
                        formatted_results.append({
                            'title': result.get('title', ''),
                            'url': result.get('url', ''),
                            'content': result.get('content', ''),
                            'engine': result.get('engine', 'searxng')
                        })
                    
                    return formatted_results
                    
                except json.JSONDecodeError as e:
                    self.logger.error(f"âŒ JSONè§£æé”™è¯¯: {str(e)}")
                    return []
                    
            else:
                self.logger.error(f"âŒ SearxNGè¯·æ±‚å¤±è´¥: {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ SearxNGæœç´¢é”™è¯¯: {str(e)}")
            return []
    
    def extract_emails_with_detailed_logging(self, text, source_info=""):
        """å¢å¼ºçš„é‚®ç®±æå– - è¯¦ç»†æ—¥å¿—"""
        emails = self.email_pattern.findall(text)
        
        if emails:
            self.logger.debug(f"ğŸ” åœ¨'{source_info[:30]}...'ä¸­å‘ç°{len(emails)}ä¸ªé‚®ç®±æ¨¡å¼")
        
        valid_emails = []
        for email in emails:
            email_lower = email.lower()
            
            # æ’é™¤è§„åˆ™
            exclusions = [
                'example.com', 'test.com', 'domain.com', 'yoursite.com',
                'noreply', 'no-reply', 'donotreply', 'privacy@', 'legal@',
                'support@example', 'admin@example', 'info@example',
                'webmaster@', 'abuse@', 'postmaster@', 'sample@'
            ]
            
            if any(pattern in email_lower for pattern in exclusions):
                self.logger.debug(f"âŒ æ’é™¤ç¤ºä¾‹é‚®ç®±: {email}")
                continue
            
            # åŸºæœ¬éªŒè¯
            if 5 < len(email) < 100 and email.count('@') == 1:
                domain = email.split('@')[1]
                if '.' in domain and len(domain) > 4:
                    valid_emails.append(email)
                    self.logger.info(f"âœ… å‘ç°æœ‰æ•ˆé‚®ç®±: {email}")
                    self.search_stats['unique_domains'].add(domain)
        
        return list(set(valid_emails))
    
    def scrape_website_with_logging(self, url):
        """å¢å¼ºçš„ç½‘ç«™çˆ¬å– - è¯¦ç»†æ—¥å¿—"""
        try:
            self.logger.info(f"ğŸŒ çˆ¬å–ç½‘ç«™: {url[:60]}...")
            self.search_stats['websites_scraped'] += 1
            
            start_time = time.time()
            response = self.session.get(url, timeout=15)
            duration = time.time() - start_time
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ç§»é™¤æ— ç”¨å…ƒç´ 
                for element in soup(["script", "style", "nav", "footer", "header"]):
                    element.decompose()
                
                # æå–æ–‡æœ¬
                text = soup.get_text()
                
                # ç‰¹åˆ«å…³æ³¨è”ç³»é¡µé¢
                contact_sections = soup.find_all(['div', 'section'], 
                    class_=re.compile(r'contact|about|team|staff', re.I))
                
                contact_text = ""
                for section in contact_sections:
                    contact_text += " " + section.get_text()
                
                all_text = text + " " + contact_text
                
                emails = self.extract_emails_with_detailed_logging(all_text, f"ç½‘ç«™ {url}")
                
                self.logger.info(f"âœ… ç½‘ç«™çˆ¬å–å®Œæˆ ({duration:.1f}s): å‘ç°{len(emails)}ä¸ªé‚®ç®±")
                return emails
                
            else:
                self.logger.warning(f"âš ï¸ ç½‘ç«™è¿”å›çŠ¶æ€ç : {response.status_code}")
                return []
                
        except Exception as e:
            self.logger.error(f"âŒ ç½‘ç«™çˆ¬å–å¤±è´¥ {url}: {str(e)}")
            return []
    
    def execute_persistent_email_search(self, industry, target_count=5):
        """æ‰§è¡ŒæŒç»­æœç´¢ç›´åˆ°æ‰¾åˆ°é‚®ç®±"""
        self.logger.info(f"ğŸš€ å¼€å§‹æŒç»­æœç´¢ - è¡Œä¸š: {industry}, ç›®æ ‡: {target_count}ä¸ªé‚®ç®±")
        self.target_email_count = target_count
        
        start_time = time.time()
        all_found_emails = []
        round_number = 1
        
        while len(all_found_emails) < target_count and round_number <= self.max_search_rounds:
            self.logger.info(f"\nğŸ“ ç¬¬{round_number}è½®æœç´¢ - å·²æ‰¾åˆ°{len(all_found_emails)}ä¸ªé‚®ç®±")
            
            # ç”Ÿæˆæœ¬è½®æœç´¢ç­–ç•¥
            strategies = self.generate_enhanced_search_strategies(industry, round_number)
            
            round_emails = []
            
            # æ‰§è¡Œæœ¬è½®æ‰€æœ‰ç­–ç•¥
            for i, strategy in enumerate(strategies, 1):
                self.logger.info(f"   ğŸ¯ ç­–ç•¥{i}/{len(strategies)}: {strategy}")
                
                # SearxNGæœç´¢
                search_results = self.search_with_enhanced_logging(strategy)
                
                if not search_results:
                    self.logger.warning(f"   âš ï¸ ç­–ç•¥{i}æ— æœç´¢ç»“æœ")
                    continue
                
                # ä»æœç´¢é¢„è§ˆæå–é‚®ç®±
                preview_emails = []
                for result in search_results:
                    text = f"{result['title']} {result['content']}"
                    emails = self.extract_emails_with_detailed_logging(text, f"æœç´¢é¢„è§ˆ-{result.get('title', '')[:20]}")
                    
                    for email in emails:
                        if not any(e['email'] == email for e in preview_emails):
                            preview_emails.append({
                                'email': email,
                                'source': 'search_preview',
                                'source_url': result['url'],
                                'source_title': result['title'],
                                'confidence': 0.8,
                                'round': round_number,
                                'strategy': strategy
                            })
                
                self.logger.info(f"   ğŸ“§ æœç´¢é¢„è§ˆå‘ç°: {len(preview_emails)}ä¸ªé‚®ç®±")
                round_emails.extend(preview_emails)
                
                # å¹¶è¡Œçˆ¬å–ç½‘ç«™
                self.logger.info(f"   ğŸŒ å¹¶è¡Œçˆ¬å–å‰8ä¸ªç½‘ç«™...")
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                    future_to_result = {
                        executor.submit(self.scrape_website_with_logging, result['url']): result 
                        for result in search_results[:8]
                    }
                    
                    for future in concurrent.futures.as_completed(future_to_result, timeout=30):
                        try:
                            result = future_to_result[future]
                            website_emails = future.result()
                            
                            for email in website_emails:
                                if not any(e['email'] == email for e in round_emails):
                                    round_emails.append({
                                        'email': email,
                                        'source': 'website_scraping',
                                        'source_url': result['url'],
                                        'source_title': result['title'],
                                        'confidence': 0.9,
                                        'round': round_number,
                                        'strategy': strategy
                                    })
                                    
                        except Exception as e:
                            continue
                
                self.logger.info(f"   âœ… ç­–ç•¥{i}å®Œæˆ: æœ¬ç­–ç•¥æ€»è®¡{len([e for e in round_emails if e['strategy'] == strategy])}ä¸ªé‚®ç®±")
                
                # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡ï¼Œç«‹å³åœæ­¢
                all_unique = {e['email']: e for e in all_found_emails + round_emails}
                if len(all_unique) >= target_count:
                    self.logger.info(f"ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡é‚®ç®±æ•°é‡ï¼")
                    break
                
                # ç­–ç•¥é—´éš”
                time.sleep(1)
            
            # åˆå¹¶æœ¬è½®ç»“æœ
            all_found_emails.extend(round_emails)
            
            # å»é‡
            unique_emails = {e['email']: e for e in all_found_emails}
            all_found_emails = list(unique_emails.values())
            
            self.logger.info(f"ğŸ“Š ç¬¬{round_number}è½®å®Œæˆ: æœ¬è½®æ–°å¢{len(round_emails)}ä¸ªï¼Œæ€»è®¡{len(all_found_emails)}ä¸ªå”¯ä¸€é‚®ç®±")
            
            # æ›´æ–°ç»Ÿè®¡
            self.search_stats['emails_found'] = len(all_found_emails)
            
            # å¦‚æœå·²è¾¾åˆ°ç›®æ ‡ï¼Œé€€å‡ºå¾ªç¯
            if len(all_found_emails) >= target_count:
                self.logger.info(f"ğŸ‰ è¾¾æˆç›®æ ‡ï¼æ‰¾åˆ°{len(all_found_emails)}ä¸ªé‚®ç®±")
                break
                
            # å¦‚æœæœ¬è½®æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é‚®ç®±ï¼Œè°ƒæ•´ç­–ç•¥
            if len(round_emails) == 0:
                self.logger.warning(f"âš ï¸ ç¬¬{round_number}è½®æœªæ‰¾åˆ°é‚®ç®±ï¼Œè°ƒæ•´æœç´¢ç­–ç•¥...")
                
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´æ¿€è¿›çš„æœç´¢ç­–ç•¥
                if round_number >= 3:
                    self.logger.info("ğŸ”„ åˆ‡æ¢åˆ°æ›´å®½æ³›çš„æœç´¢ç­–ç•¥...")
            
            round_number += 1
            
            # è½®æ¬¡é—´éš”
            if round_number <= self.max_search_rounds:
                self.logger.info(f"â¸ï¸ è½®æ¬¡é—´éš”3ç§’...")
                time.sleep(3)
        
        # æœ€ç»ˆç»“æœ
        final_emails = all_found_emails[:target_count]
        total_time = time.time() - start_time
        
        self.logger.info(f"\nğŸŠ æŒç»­æœç´¢å®Œæˆï¼")
        self.logger.info(f"   ğŸ“§ æœ€ç»ˆé‚®ç®±æ•°: {len(final_emails)}")
        self.logger.info(f"   ğŸ”„ æœç´¢è½®æ•°: {round_number-1}")
        self.logger.info(f"   â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’")
        self.logger.info(f"   ğŸ“Š æœç´¢ç»Ÿè®¡:")
        self.logger.info(f"      - æ€»æŸ¥è¯¢æ•°: {self.search_stats['total_queries']}")
        self.logger.info(f"      - æˆåŠŸæŸ¥è¯¢: {self.search_stats['successful_queries']}")
        self.logger.info(f"      - çˆ¬å–ç½‘ç«™æ•°: {self.search_stats['websites_scraped']}")
        self.logger.info(f"      - å‘ç°åŸŸåæ•°: {len(self.search_stats['unique_domains'])}")
        
        # è½¬æ¢ç»Ÿè®¡æ•°æ®ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
        stats_dict = dict(self.search_stats)
        stats_dict['unique_domains'] = list(self.search_stats['unique_domains'])
        
        return {
            'success': len(final_emails) > 0,
            'emails': [e['email'] for e in final_emails],
            'email_details': final_emails,
            'total_emails': len(final_emails),
            'search_rounds': round_number - 1,
            'execution_time': total_time,
            'search_stats': stats_dict,
            'industry': industry,
            'target_achieved': len(final_emails) >= target_count,
            'method': 'persistent_ollama_searxng',
            'timestamp': datetime.now().isoformat()
        }

def main():
    if len(sys.argv) < 2:
        print('è¯·æä¾›è¡Œä¸šåç§°ï¼Œä¾‹å¦‚: python3 OptimizedOllamaEmailFinder.py "AI/Machine Learning" 5')
        return
    
    industry = sys.argv[1]
    target_count = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    finder = OptimizedOllamaEmailFinder()
    results = finder.execute_persistent_email_search(industry, target_count)
    
    print("\n" + "="*80)
    print("ğŸ¯ ä¼˜åŒ–é‚®ç®±æœç´¢å™¨ - æœ€ç»ˆç»“æœ")
    print("="*80)
    
    if results['success']:
        print(f"âœ… æˆåŠŸæ‰¾åˆ° {results['total_emails']} ä¸ªé‚®ç®±:")
        for i, email in enumerate(results['emails'], 1):
            print(f"   {i}. {email}")
        
        print(f"\nğŸ“Š æœç´¢æ€§èƒ½:")
        print(f"   ğŸ”„ æœç´¢è½®æ•°: {results['search_rounds']}")
        print(f"   â±ï¸ æ€»è€—æ—¶: {results['execution_time']:.1f}ç§’")
        print(f"   ğŸ¯ ç›®æ ‡è¾¾æˆ: {'æ˜¯' if results['target_achieved'] else 'å¦'}")
        print(f"   ğŸ“ˆ æˆåŠŸç‡: {results['search_stats']['successful_queries']}/{results['search_stats']['total_queries']} æŸ¥è¯¢")
        print(f"   ğŸŒ çˆ¬å–ç½‘ç«™: {results['search_stats']['websites_scraped']} ä¸ª")
        
    else:
        print("âŒ æœªèƒ½æ‰¾åˆ°é‚®ç®±")
    
    # è¾“å‡ºè¯¦ç»†JSONç»“æœ
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ (JSON):")
    print(json.dumps(results, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()