#!/usr/bin/env python3
"""
Local Ollama Email Finder
å®Œå…¨æœ¬åœ°çš„AIé‚®ç®±å‘ç°ç³»ç»Ÿ - ä¸ä½¿ç”¨ä»»ä½•å¤–éƒ¨API
- åŸºäºæœ¬åœ°Ollama LLM
- æ™ºèƒ½ç”Ÿæˆæœç´¢ç­–ç•¥å’ŒæŸ¥è¯¢
- æœ¬åœ°ç½‘ç»œçˆ¬è™« + AIåˆ†æ
- å®Œå…¨è‡ªä¸»ï¼Œä¸ä¾èµ–å¤–éƒ¨æœåŠ¡
"""

import sys
import json
import time
import re
import requests
import os
from datetime import datetime
from urllib.parse import urlparse, quote, urljoin
from bs4 import BeautifulSoup
import concurrent.futures
import random

class LocalOllamaEmailFinder:
    def __init__(self):
        # æœ¬åœ°Ollamaé…ç½®
        self.ollama_url = 'http://localhost:11434'
        
        # æœ¬åœ°çˆ¬è™«é…ç½®
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
        
        # é‚®ç®±åŒ¹é…æ¨¡å¼
        self.email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
        
        print("ğŸ¤– Local Ollama Email Finder å·²åˆå§‹åŒ–")
        print("   ğŸ§  AIå¼•æ“: æœ¬åœ°Ollama (å®Œå…¨ç¦»çº¿)")
        print("   ğŸ” æœç´¢å¼•æ“: æœ¬åœ°æ™ºèƒ½çˆ¬è™«")
        print("   ğŸ“§ é‚®ç®±å‘ç°: AIæŒ‡å¯¼çš„ç½‘ç»œæœç´¢")
        print("   ğŸš« å¤–éƒ¨API: é›¶ä¾èµ–")
        print("   âš¡ ä¼˜åŠ¿: å®Œå…¨è‡ªä¸»ï¼Œéšç§å®‰å…¨")
        
    def ask_ollama(self, prompt, model='qwen2.5:0.5b', temperature=0.7):
        """ä¸æœ¬åœ°Ollamaäº¤äº’çš„é€šç”¨æ–¹æ³•"""
        try:
            response = requests.post(f"{self.ollama_url}/api/generate", json={
                'model': model,
                'prompt': prompt,
                'stream': False,
                'options': {
                    'temperature': temperature,
                    'num_ctx': 2048
                }
            }, timeout=30)
            
            if response.status_code == 200:
                return response.json()['response'].strip()
            else:
                print(f"âš ï¸  Ollama APIé”™è¯¯: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ Ollamaè¯·æ±‚å¤±è´¥: {str(e)}")
            return None
    
    def generate_search_strategy(self, industry):
        """è®©æœ¬åœ°Ollamaç”Ÿæˆæ™ºèƒ½æœç´¢ç­–ç•¥"""
        print(f"ğŸ§  æ­£åœ¨ä¸º'{industry}'è¡Œä¸šç”Ÿæˆæœç´¢ç­–ç•¥...")
        
        prompt = f"""ä¸º{industry}è¡Œä¸šç”Ÿæˆç®€çŸ­é«˜æ•ˆçš„é‚®ç®±æœç´¢ç­–ç•¥ã€‚

è¦æ±‚:
- æœç´¢è¯ç»„å¿…é¡»ç®€çŸ­(æœ€å¤š3-4ä¸ªè¯)
- ä¸ä½¿ç”¨å¤æ‚æ“ä½œç¬¦(site:, intext:, filetype:ç­‰)
- ç›´æ¥æœ‰æ•ˆçš„å…³é”®è¯ç»„åˆ

è¿”å›JSONæ ¼å¼:
{{
  "search_queries": [
    "{industry} CEO email",
    "{industry} founder contact",
    "{industry} business email",
    "{industry} company contact",
    "{industry} sales email"
  ],
  "website_types": [
    "ç½‘ç«™ç±»å‹1",
    "ç½‘ç«™ç±»å‹2", 
    "ç½‘ç«™ç±»å‹3"
  ],
  "industry_keywords": [
    "å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3", "å…³é”®è¯4", "å…³é”®è¯5"
  ]
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—:"""

        response = self.ask_ollama(prompt, temperature=0.3)
        
        if response:
            try:
                # æå–JSON
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    strategy = json.loads(response[json_start:json_end])
                    
                    print(f"   âœ… ç”Ÿæˆäº†æœç´¢ç­–ç•¥:")
                    print(f"      ğŸ“ æœç´¢æŸ¥è¯¢: {len(strategy.get('search_queries', []))}ä¸ª")
                    print(f"      ğŸŒ ç½‘ç«™ç±»å‹: {len(strategy.get('website_types', []))}ä¸ª")
                    print(f"      ğŸ”‘ è¡Œä¸šå…³é”®è¯: {len(strategy.get('industry_keywords', []))}ä¸ª")
                    
                    return strategy
            except json.JSONDecodeError:
                pass
        
        # å¤‡ç”¨ç­–ç•¥
        print("   âš ï¸  ä½¿ç”¨å¤‡ç”¨æœç´¢ç­–ç•¥")
        return {
            "search_queries": [
                f"{industry} company CEO founder email contact",
                f"{industry} business sales email address",
                f"{industry} company contact us email",
                f"{industry} partnership business development email",
                f"{industry} customer support contact email"
            ],
            "website_types": [
                "å…¬å¸å®˜ç½‘",
                "è¡Œä¸šç›®å½•", 
                "å•†ä¸šå¹³å°"
            ],
            "industry_keywords": [
                industry, "business", "company", "contact", "email"
            ]
        }
    
    def search_web_intelligently(self, query, max_results=10):
        """æ™ºèƒ½ç½‘ç»œæœç´¢ - ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“"""
        print(f"   ğŸ” æœç´¢: {query}")
        
        search_engines = [
            {
                'name': 'DuckDuckGo',
                'url': 'https://duckduckgo.com/html/',
                'params': {'q': query}
            },
            {
                'name': 'Bing', 
                'url': 'https://www.bing.com/search',
                'params': {'q': query, 'count': max_results}
            }
        ]
        
        all_urls = []
        
        for engine in search_engines:
            try:
                print(f"      ğŸ“¡ å°è¯•{engine['name']}...")
                
                response = self.session.get(
                    engine['url'], 
                    params=engine['params'], 
                    timeout=10
                )
                
                if response.status_code == 200:
                    urls = self.extract_urls_from_search_results(response.text, engine['name'])
                    all_urls.extend(urls)
                    print(f"         âœ… æ‰¾åˆ°{len(urls)}ä¸ªURLs")
                    
                    if len(all_urls) >= max_results:
                        break
                else:
                    print(f"         âŒ {engine['name']} å¤±è´¥: {response.status_code}")
                
                # æœç´¢å¼•æ“é—´å»¶è¿Ÿ
                time.sleep(random.uniform(2, 4))
                
            except Exception as e:
                print(f"         âŒ {engine['name']} é”™è¯¯: {str(e)}")
                continue
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_urls = list(set(all_urls))[:max_results]
        print(f"      ğŸ“Š æ€»å…±æ”¶é›†åˆ°{len(unique_urls)}ä¸ªå”¯ä¸€URLs")
        
        return unique_urls
    
    def extract_urls_from_search_results(self, html, engine_name):
        """ä»æœç´¢ç»“æœé¡µé¢æå–URLs"""
        soup = BeautifulSoup(html, 'html.parser')
        urls = []
        
        try:
            if engine_name == 'DuckDuckGo':
                # DuckDuckGoç»“æœé€‰æ‹©å™¨
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if href.startswith('http') and 'duckduckgo.com' not in href:
                        urls.append(href)
                        
            elif engine_name == 'Bing':
                # Bingç»“æœé€‰æ‹©å™¨
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if href.startswith('http') and 'bing.com' not in href and 'microsoft.com' not in href:
                        urls.append(href)
            
        except Exception as e:
            print(f"         âš ï¸  URLæå–é”™è¯¯: {str(e)}")
        
        return urls[:15]  # é™åˆ¶æ¯ä¸ªæœç´¢å¼•æ“çš„ç»“æœæ•°é‡
    
    def scrape_website_for_emails(self, url):
        """çˆ¬å–ç½‘ç«™å¯»æ‰¾é‚®ç®±"""
        try:
            print(f"      ğŸŒ çˆ¬å–: {url[:50]}...")
            
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ç§»é™¤æ— ç”¨å…ƒç´ 
                for element in soup(["script", "style", "nav", "footer"]):
                    element.decompose()
                
                # æå–æ–‡æœ¬
                text = soup.get_text()
                
                # æŸ¥æ‰¾é‚®ç®±
                emails = self.email_pattern.findall(text)
                
                # è¿‡æ»¤æœ‰æ•ˆé‚®ç®±
                valid_emails = []
                for email in emails:
                    if self.is_valid_business_email(email):
                        valid_emails.append(email)
                
                if valid_emails:
                    print(f"         âœ… æ‰¾åˆ°{len(valid_emails)}ä¸ªé‚®ç®±")
                    
                    # å°è¯•è·å–é¡µé¢æ ‡é¢˜ä½œä¸ºå…¬å¸å
                    title_elem = soup.find('title')
                    page_title = title_elem.get_text().strip() if title_elem else ''
                    company_name = self.extract_company_name_from_title(page_title, url)
                    
                    return {
                        'url': url,
                        'emails': valid_emails,
                        'company_name': company_name,
                        'page_title': page_title,
                        'content_snippet': text[:300].replace('\n', ' ').strip()
                    }
                else:
                    print(f"         âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆé‚®ç®±")
                    return None
                    
            else:
                print(f"         âŒ è®¿é—®å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"         âŒ çˆ¬å–é”™è¯¯: {str(e)}")
            return None
    
    def is_valid_business_email(self, email):
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆå•†ä¸šé‚®ç®±"""
        email_lower = email.lower()
        
        # æ’é™¤å‡é‚®ç®±
        invalid_patterns = [
            'example.com', 'test.com', 'domain.com', 'yoursite.com',
            'noreply', 'no-reply', 'donotreply', 'privacy@', 'legal@',
            'support@example', 'admin@example', 'info@example'
        ]
        
        if any(pattern in email_lower for pattern in invalid_patterns):
            return False
        
        # åŸºæœ¬æ ¼å¼éªŒè¯
        if 5 < len(email) < 100 and email.count('@') == 1:
            domain = email.split('@')[1]
            if '.' in domain and len(domain) > 4:
                return True
        
        return False
    
    def extract_company_name_from_title(self, title, url):
        """ä»é¡µé¢æ ‡é¢˜æˆ–URLæå–å…¬å¸å"""
        try:
            if title:
                # æ¸…ç†æ ‡é¢˜
                cleaned_title = title.split('|')[0].split('-')[0].strip()
                if len(cleaned_title) > 0 and len(cleaned_title) < 50:
                    return cleaned_title
            
            # ä»URLæå–
            parsed = urlparse(url)
            domain = parsed.netloc.replace('www.', '')
            company = domain.split('.')[0]
            return company.title()
            
        except:
            return 'Unknown Company'
    
    def analyze_emails_with_ollama(self, email_results):
        """ä½¿ç”¨æœ¬åœ°Ollamaåˆ†æå‘ç°çš„é‚®ç®±å¹¶ç”Ÿæˆprofiles"""
        print(f"   ğŸ§  ä½¿ç”¨Ollamaåˆ†æ{len(email_results)}ä¸ªé‚®ç®±ç»“æœ...")
        
        analyzed_emails = []
        
        for result in email_results:
            for email in result['emails']:
                try:
                    print(f"      ğŸ‘¤ åˆ†æé‚®ç®±: {email}")
                    
                    prompt = f"""åˆ†æè¿™ä¸ªé‚®ç®±å¹¶ç”Ÿæˆä¸“ä¸šprofile:

é‚®ç®±: {email}
å…¬å¸: {result['company_name']}
ç½‘ç«™: {result['url']}
é¡µé¢æ ‡é¢˜: {result['page_title']}
å†…å®¹æ‘˜è¦: {result['content_snippet']}

åŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆprofileï¼Œè¿”å›JSON:
{{
  "name": "æ¨æµ‹çš„å…¨å",
  "email": "{email}",
  "title": "æ¨æµ‹çš„èŒä½",
  "company": "{result['company_name']}",
  "industry": "æ¨æµ‹çš„è¡Œä¸š",
  "background": "ç®€è¦èƒŒæ™¯æè¿°",
  "confidence": "ä¿¡å¿ƒåº¦(0.1-1.0)"
}}

åªè¿”å›JSON:"""

                    response = self.ask_ollama(prompt, model='llama3.2', temperature=0.3)
                    
                    if response:
                        try:
                            json_start = response.find('{')
                            json_end = response.rfind('}') + 1
                            if json_start != -1:
                                profile = json.loads(response[json_start:json_end])
                                
                                # æ·»åŠ é¢å¤–ä¿¡æ¯
                                profile.update({
                                    'source_url': result['url'],
                                    'page_title': result['page_title'],
                                    'content_snippet': result['content_snippet'],
                                    'found_at': datetime.now().isoformat(),
                                    'analysis_method': 'local_ollama'
                                })
                                
                                analyzed_emails.append(profile)
                                print(f"         âœ… ç”Ÿæˆprofile: {profile.get('name', 'Unknown')}")
                                
                        except json.JSONDecodeError:
                            print(f"         âš ï¸  Profileç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯")
                            # åŸºç¡€profile
                            email_prefix = email.split('@')[0]
                            basic_profile = {
                                'name': email_prefix.replace('.', ' ').replace('_', ' ').title(),
                                'email': email,
                                'title': 'Professional',
                                'company': result['company_name'],
                                'industry': 'Business',
                                'background': f"Professional at {result['company_name']}",
                                'confidence': 0.5,
                                'source_url': result['url'],
                                'found_at': datetime.now().isoformat(),
                                'analysis_method': 'basic_extraction'
                            }
                            analyzed_emails.append(basic_profile)
                    
                    # åˆ†æé—´éš”
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"         âŒ é‚®ç®±åˆ†æå¤±è´¥: {str(e)}")
                    continue
        
        return analyzed_emails
    
    def find_emails_with_local_ai(self, industry, max_emails=5):
        """ä½¿ç”¨æœ¬åœ°AIè¿›è¡Œé‚®ç®±å‘ç°çš„ä¸»æ–¹æ³•"""
        print(f"ğŸ¤– å¯åŠ¨æœ¬åœ°AIé‚®ç®±å‘ç°: {industry}")
        print(f"ğŸ¯ ç›®æ ‡: {max_emails}ä¸ªé‚®ç®±")
        print("=" * 60)
        
        # 1. ç”Ÿæˆæœç´¢ç­–ç•¥
        strategy = self.generate_search_strategy(industry)
        
        all_email_results = []
        all_analyzed_profiles = []
        
        # 2. æ‰§è¡Œæ¯ä¸ªæœç´¢æŸ¥è¯¢
        for i, query in enumerate(strategy['search_queries'], 1):
            print(f"\nğŸ“ æ‰§è¡Œæœç´¢ç­–ç•¥ {i}/{len(strategy['search_queries'])}")
            print(f"   ğŸ” æŸ¥è¯¢: {query}")
            
            try:
                # æœç´¢ç½‘ç»œ
                urls = self.search_web_intelligently(query, max_results=8)
                
                if urls:
                    print(f"   ğŸ“„ æ‰¾åˆ°{len(urls)}ä¸ªå€™é€‰ç½‘ç«™ï¼Œå¼€å§‹çˆ¬å–...")
                    
                    # å¹¶è¡Œçˆ¬å–ç½‘ç«™
                    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                        future_to_url = {
                            executor.submit(self.scrape_website_for_emails, url): url 
                            for url in urls[:6]  # é™åˆ¶å¹¶å‘æ•°é‡
                        }
                        
                        for future in concurrent.futures.as_completed(future_to_url):
                            try:
                                result = future.result()
                                if result and result['emails']:
                                    all_email_results.append(result)
                                    print(f"      âœ… ä»{result['company_name']}æ‰¾åˆ°{len(result['emails'])}ä¸ªé‚®ç®±")
                                    
                                    # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡
                                    total_emails = sum(len(r['emails']) for r in all_email_results)
                                    if total_emails >= max_emails:
                                        print(f"   ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡é‚®ç®±æ•°é‡")
                                        break
                                        
                            except Exception as e:
                                print(f"      âŒ çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}")
                
                # æŸ¥è¯¢é—´éš”
                time.sleep(random.uniform(3, 6))
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¶³å¤Ÿé‚®ç®±
                total_emails = sum(len(r['emails']) for r in all_email_results)
                if total_emails >= max_emails:
                    break
                    
            except Exception as e:
                print(f"   âŒ æœç´¢ç­–ç•¥{i}å¤±è´¥: {str(e)}")
                continue
        
        # 3. ä½¿ç”¨Ollamaåˆ†ææ‰€æœ‰å‘ç°çš„é‚®ç®±
        if all_email_results:
            print(f"\nğŸ§  ä½¿ç”¨æœ¬åœ°Ollamaåˆ†æå‘ç°çš„é‚®ç®±...")
            all_analyzed_profiles = self.analyze_emails_with_ollama(all_email_results)
        
        # 4. æ•´ç†ç»“æœ
        unique_emails = []
        seen_emails = set()
        
        for profile in all_analyzed_profiles:
            if profile['email'] not in seen_emails:
                unique_emails.append(profile)
                seen_emails.add(profile['email'])
        
        # é™åˆ¶åˆ°ç›®æ ‡æ•°é‡
        final_results = unique_emails[:max_emails]
        
        print(f"\nğŸ‰ æœ¬åœ°AIé‚®ç®±å‘ç°å®Œæˆ!")
        print(f"   ğŸ“§ å‘ç°çš„å”¯ä¸€é‚®ç®±: {len(final_results)}")
        print(f"   ğŸ§  AIåˆ†æçš„profiles: {len(final_results)}")
        print(f"   ğŸ” ä½¿ç”¨çš„æœç´¢ç­–ç•¥: {len(strategy['search_queries'])}")
        print(f"   ğŸŒ çˆ¬å–çš„ç½‘ç«™: {len(all_email_results)}")
        
        return {
            'emails': final_results,
            'strategy': strategy,
            'total_websites_scraped': len(all_email_results),
            'total_emails_found': len(final_results),
            'search_method': 'local_ollama_ai'
        }

def main():
    if len(sys.argv) < 2:
        print(json.dumps({'error': 'è¯·æä¾›è¡Œä¸šåç§° (ä¾‹å¦‚: "AI startups", "fruit companies")'}))
        return
    
    industry = sys.argv[1]
    max_emails = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    
    finder = LocalOllamaEmailFinder()
    
    # æ‰§è¡Œæœ¬åœ°AIé‚®ç®±å‘ç°
    results = finder.find_emails_with_local_ai(industry, max_emails)
    
    # å‡†å¤‡è¾“å‡º
    output = {
        'emails': results['emails'],
        'strategy_used': results['strategy'],
        'total_emails': results['total_emails_found'],
        'websites_scraped': results['total_websites_scraped'],
        'industry': industry,
        'search_method': 'local_ollama_ai',
        'completely_local': True,
        'no_external_apis': True,
        'timestamp': datetime.now().isoformat()
    }
    
    print("\n" + "=" * 60)
    print("ğŸ¤– æœ¬åœ°OLLAMA AIé‚®ç®±å‘ç°ç»“æœ")
    print("=" * 60)
    
    if results['emails']:
        print("ğŸ“§ å‘ç°çš„é‚®ç®±å’ŒProfiles:")
        for profile in results['emails']:
            print(f"   ğŸ“§ {profile['email']}")
            print(f"      ğŸ‘¤ {profile.get('name', 'Unknown')} - {profile.get('title', 'Unknown')}")
            print(f"      ğŸ¢ {profile.get('company', 'Unknown')}")
            print(f"      ğŸŒ æ¥æº: {profile.get('source_url', 'Unknown')}")
            print()
    
    print(f"ğŸ¯ æœç´¢ç­–ç•¥:")
    for i, query in enumerate(results['strategy']['search_queries'], 1):
        print(f"   {i}. {query}")
    
    print(f"\nğŸ“Š ç»Ÿè®¡:")
    print(f"   ğŸ“§ å‘ç°é‚®ç®±: {results['total_emails_found']}")
    print(f"   ğŸŒ çˆ¬å–ç½‘ç«™: {results['total_websites_scraped']}")
    print(f"   ğŸš« å¤–éƒ¨API: 0 (å®Œå…¨æœ¬åœ°)")
    
    print(json.dumps(output, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()